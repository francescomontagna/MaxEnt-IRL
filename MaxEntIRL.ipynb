{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMfzN78LGWdraRXhL9zSGQO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescomontagna/MaxEnt-IRL/blob/main/MaxEntIRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SImPGvTum7X"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYcfdDjhuvZG"
      },
      "source": [
        "class GridWorld:\n",
        "  def __init__(self, p_initial, grid_size = 5):\n",
        "    self.actions = [0, 1, 2, 3] # [up, right, down, left]\n",
        "    self.grid_size = grid_size\n",
        "    self.terminal_state = (4, 5)\n",
        "    self.p_initial = p_initial # must be computed form trajectories\n",
        "\n",
        "    self.reset()\n",
        "\n",
        "  def state_coord_to_point(self, state_coord):\n",
        "    return state_coord[0]*self.grid_size + state_coord[1]\n",
        "\n",
        "  # TODO: test\n",
        "  def point_to_state_coord(self, point):\n",
        "    row = point // self.grid_size\n",
        "    col = point % grid_size\n",
        "    return row, col\n",
        "\n",
        "  #TODO: test\n",
        "  def simulate_step(self, state, action):\n",
        "    s_coord = self.point_to_state_coord(state)\n",
        "    next_s_coord = None\n",
        "\n",
        "    if action == 0:\n",
        "      if s_coord[0] == 0:\n",
        "        next_s_coord = s_coord\n",
        "      else:\n",
        "        next_s_coord = (s_coord[0]-1, s_coord[1])\n",
        "\n",
        "    elif action == 1:\n",
        "      if s_coord[1] == (self.grid_size-1):\n",
        "        next_s_coord = s_coord\n",
        "      else:\n",
        "        next_s_coord = (s_coord[0], s_coord[1]+1)\n",
        "      \n",
        "    elif action == 2:\n",
        "      if s_coord[0] == (self.grid_size-1):\n",
        "        next_s_coord = s_coord\n",
        "      else:\n",
        "        next_s_coord = (s_coord[0]+1, s_coord[1])\n",
        "\n",
        "    elif action == 3:\n",
        "      if s_coord[1] == 0:\n",
        "        next_s_coord = s_coord\n",
        "      else:\n",
        "        next_s_coord = (s_coord[0], s_coord[1]-1)\n",
        "\n",
        "    return self.state_coord_to_point(next_s_coord)\n",
        "\n",
        "  def step(self, state, action):\n",
        "    next_state = self.simulate_step(state, action)\n",
        "    self.state_coord = self.point_to_state_coord(next_state) \n",
        "\n",
        "    return self.state\n",
        "\n",
        "  # Initialize the gridworld\n",
        "  def reset(self):\n",
        "    initial_state = random.choice(range(self.grid_size**2), p=self.p_initial)\n",
        "    self.state_coord = self.state_coord_to_point(initial_state)\n",
        "    "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_DeRVGZxKJC"
      },
      "source": [
        "class MaxCausalEntropy:\n",
        "  # We'll consider the case of a deterministic policy\n",
        "  def __init__(self, trajectories, lr, decay):\n",
        "    \"\"\"\n",
        "    Attributes:\n",
        "      trajectories: list of the sampeld trajectories\n",
        "      lr: learning rate for SGA\n",
        "      decay: weight decay for SGA\n",
        "      world: The environment modeled as GridWorld\n",
        "      features: feature to represrent each state. Simply encoded as coordinates in the grid\n",
        "      theta: learnable parameters, one vector for each possible state. Same dimensionality of features\n",
        "      V: (n_states x n_states) matrix with value associated to each state\n",
        "      D: (n_states x n_states) matrix with visitation frequencies\n",
        "      gamma: discount (Forse dev'essere ritornato da simulate_step, decido poi)\n",
        "    \"\"\"\n",
        "    self.trajectories = trajectories\n",
        "    self.lr = lr\n",
        "    self.decay = decay\n",
        "\n",
        "    self.world = GridWorld()\n",
        "    self.n_states = self.world.grid_size**2\n",
        "    self.features = np.zeros((self.n_states, 2))\n",
        "    for s in range(self.n_states):\n",
        "      i, j = self.world.point_to_state_coord(s)\n",
        "      self.features[s, 0] = i\n",
        "      self.features[s, 1] = j\n",
        "\n",
        "    self.theta = np.zeros((self.n_states, 2)) # np.matmul(self.theta[i, j], self.features[i, j]) = R(s=[i,j])\n",
        "    self.V = np.zeros((self.n_states))\n",
        "    self.D = np.zeros((self.n_states))\n",
        "\n",
        "    self.gamma = 0.9\n",
        "    \n",
        "\n",
        "  def R(self, state):\n",
        "    \"\"\"\n",
        "    Return reward associated to given state (point)\n",
        "    \"\"\"\n",
        "    return np.matmul(self.theta[s], self.features[s])\n",
        "\n",
        "  # def Q(self, state, action):\n",
        "  #   \"\"\"\n",
        "  #   Return action value associated to given state (point)\n",
        "  #   Maximize over action to get Q(S, A)\n",
        "  #   \"\"\"\n",
        "  #   next_s = self.world.simulate_step(state, action)\n",
        "  #   return self.R(state) + self.gamma * self.V[next_s]\n",
        "\n",
        "  def policy(self, state, action):\n",
        "    return np.exp(self.Q(state, action) - self.V[state])\n",
        "    \n",
        "  def feature_exp_from_trajectories(self):\n",
        "    # Compute feature_exp, as a representation of expert behaviour\n",
        "    _, features_dim = self.features.shape\n",
        "    f_exp = np.zeros((features_dim))\n",
        "\n",
        "    for trajectory in self.trajectories:\n",
        "      for step in trajectory:\n",
        "        state, action = step\n",
        "        f_exp += self.features[state] # TODO: check if it's point or coord\n",
        "\n",
        "    return f_exp / len(self.trajectories)\n",
        "\n",
        "\n",
        "  def pseudo_gpi(self):\n",
        "    \"\"\"\n",
        "    Algorithm to update the value function\n",
        "    \"\"\"\n",
        "    # NOTE: state = point in range(self.num_states)\n",
        "    eps = 1e-3\n",
        "    \n",
        "    # For each state, define set V to -inf\n",
        "    self.V = np.ones((self.n_states))*np.min\n",
        "\n",
        "    # Set V_prime to -inf except for termnal state which is set to 0\n",
        "    V_prime = np.ones((self.n_states))*np.min\n",
        "    v_prime[self.world.state_coord_to_point(self.terminal_state)] = 0\n",
        "\n",
        "    delta = np.ones((self.num_states))*np.max # parameter to monitor convergence\n",
        "    while np.max(delta) > eps:\n",
        "      # Update V for each state\n",
        "      for s in range(self.num_states):\n",
        "        for a in self.world.actions:\n",
        "          next_s = self.world.simulate_step(s, a)\n",
        "          V_prime[s] = np.log(np.exp(V_prime[s]) + np.exp(self.R(s) + self.V[next_s]))\n",
        "\n",
        "        delta[s] = np.abs(self.V[s] - V_prime[s])\n",
        "        self.V[s] = V_prime[s]\n",
        "\n",
        "    # return self.V \n",
        "\n",
        "  def expected_svf(self):\n",
        "    eps = 1e-5\n",
        "\n",
        "    self.D = np.zeros((n_states))\n",
        "    D_prime = world.p_initial # TODO Handle copy (shallow, deep, .. boh)\n",
        "\n",
        "    delta = np.ones((self.num_states))*np.max\n",
        "    while np.max(delta) > eps\n",
        "      for s in range(self.num_states):\n",
        "        for a in self.world.actions:\n",
        "          next_s = self.world.simulate_step(s, a)\n",
        "          D_prime[s] += self.D[s]*self.policy(next_s, a)\n",
        "\n",
        "        delta[s] = np.abs(self.D[s] = D_prime[s])\n",
        "        self.D[s] = D_prime[s]\n",
        "           \n",
        "    # return self.D[s]\n",
        "\n",
        "  def sga(self):\n",
        "    eps = 1e-4\n",
        "    delta = np.zeros(self.features.shape)\n",
        "    while np.max(delta) > eps\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}