{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOdb+exUiTDkussO2sVvrXY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescomontagna/MaxEnt-IRL/blob/main/Basic-IRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SImPGvTum7X"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from itertools import product"
      ],
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkDx9n_kE_Dm"
      },
      "source": [
        "# Classic RL\n",
        "Use TD Learning to learn a policy that will be used as the expert policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA3lPnivGFjt"
      },
      "source": [
        "def run_experiment(env, agent, number_of_steps):\n",
        "    mean_reward = 0.\n",
        "    try:\n",
        "      action = agent.initial_action()\n",
        "    except AttributeError:\n",
        "      action = 0\n",
        "    for i in range(number_of_steps):\n",
        "      reward, discount, next_state = grid.step(action)\n",
        "      action = agent.step(reward, discount, next_state)\n",
        "      mean_reward += (reward - mean_reward)/(i + 1.)\n",
        "\n",
        "    return mean_reward\n",
        "\n",
        "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
        "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
        "\n",
        "def plot_values(values, colormap='pink', vmin=0, vmax=10):\n",
        "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  plt.colorbar(ticks=[vmin, vmax])\n",
        "\n",
        "def plot_action_values(action_values, vmin=0, vmax=10):\n",
        "  q = action_values\n",
        "  fig = plt.figure(figsize=(8, 8))\n",
        "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "  for a in [0, 1, 2, 3]:\n",
        "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
        "    plot_values(q[..., a], vmin=vmin, vmax=vmax)\n",
        "    action_name = map_from_action_to_name(a)\n",
        "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
        "    \n",
        "  plt.subplot(3, 3, 5)\n",
        "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
        "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
        "  plt.title(\"$v(s)$\")\n",
        "\n",
        "\n",
        "def plot_rewards(xs, rewards, color):\n",
        "  mean = np.mean(rewards, axis=0)\n",
        "  p90 = np.percentile(rewards, 90, axis=0)\n",
        "  p10 = np.percentile(rewards, 10, axis=0)\n",
        "  plt.plot(xs, mean, color=color, alpha=0.6)\n",
        "  plt.fill_between(xs, p90, p10, color=color, alpha=0.3)\n",
        "  \n",
        "\n",
        "def parameter_study(parameter_values, parameter_name,\n",
        "  agent_constructor, env_constructor, color, repetitions=10, number_of_steps=int(1e4)):\n",
        "  mean_rewards = np.zeros((repetitions, len(parameter_values)))\n",
        "  greedy_rewards = np.zeros((repetitions, len(parameter_values)))\n",
        "  for rep in range(repetitions):\n",
        "    for i, p in enumerate(parameter_values):\n",
        "      env = env_constructor()\n",
        "      agent = agent_constructor()\n",
        "      if 'eps' in parameter_name:\n",
        "        agent.set_epsilon(p)\n",
        "      elif 'alpha' in parameter_name:\n",
        "        agent._step_size = p\n",
        "      else:\n",
        "        raise NameError(\"Unknown parameter_name: {}\".format(parameter_name))\n",
        "      mean_rewards[rep, i] = run_experiment(grid, agent, number_of_steps)\n",
        "      agent.set_epsilon(0.)\n",
        "      agent._step_size = 0.\n",
        "      greedy_rewards[rep, i] = run_experiment(grid, agent, number_of_steps//10)\n",
        "      del env\n",
        "      del agent\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plot_rewards(parameter_values, mean_rewards, color)\n",
        "  plt.yticks=([0, 1], [0, 1])\n",
        "  # plt.ylim((0, 1.5))\n",
        "  plt.ylabel(\"Average reward over first {} steps\".format(number_of_steps), size=12)\n",
        "  plt.xlabel(parameter_name, size=12)\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plot_rewards(parameter_values, greedy_rewards, color)\n",
        "  plt.yticks=([0, 1], [0, 1])\n",
        "  # plt.ylim((0, 1.5))\n",
        "  plt.ylabel(\"Final rewards, with greedy policy\".format(number_of_steps), size=12)\n",
        "  plt.xlabel(parameter_name, size=12)\n",
        "  \n",
        "def epsilon_greedy(q_values, epsilon):\n",
        "  if epsilon < np.random.random():\n",
        "    return np.argmax(q_values)\n",
        "  else:\n",
        "    return np.random.randint(np.array(q_values).shape[-1])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYgvXUEJFJXi"
      },
      "source": [
        "class Grid(object):\n",
        "\n",
        "  def __init__(self, noisy=False):\n",
        "    # -1: wall\n",
        "    # 0: empty, episode continues\n",
        "    # other: number indicates reward, episode will terminate\n",
        "    self._layout = np.array([\n",
        "      [-1, -1, -1, -1, -1, -1, -1],\n",
        "      [-1,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0, 10, -1],\n",
        "      [-1,  0,  0,  0,  0,  0, -1],\n",
        "      [-1, -1, -1, -1, -1, -1, -1],\n",
        "    ])\n",
        "    self._start_state = random.choices(range(1, self._layout.shape[0]-1), k=2)\n",
        "    while self._start_state == [4, 5]:\n",
        "      self._start_state = random.choices(range(1, self._layout.shape[0]-1), k=2)\n",
        "    self._state = self._start_state\n",
        "    self._number_of_states = np.prod(np.shape(self._layout))\n",
        "    self._noisy = noisy\n",
        "\n",
        "  @property\n",
        "  def number_of_states(self):\n",
        "      return self._number_of_states\n",
        "    \n",
        "  def plot_grid(self):\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(self._layout > -1, interpolation=\"nearest\", cmap='pink')\n",
        "    ax = plt.gca() # 'get current axis'\n",
        "    ax.grid(0)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title(\"The grid\")\n",
        "    plt.text(2, 2, r\"$\\mathbf{S}$\", ha='center', va='center')\n",
        "    plt.text(8, 3, r\"$\\mathbf{G}$\", ha='center', va='center')\n",
        "    h, w = self._layout.shape\n",
        "    for y in range(h-1):\n",
        "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
        "    for x in range(w-1):\n",
        "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n",
        "\n",
        "  \n",
        "  def get_obs(self):\n",
        "    y, x = self._state\n",
        "    return y*self._layout.shape[1] + x\n",
        "\n",
        "  def obs_to_state(obs):\n",
        "    x = obs % self._layout.shape[1]\n",
        "    y = obs // self._layout.shape[1]\n",
        "    s = np.copy(grid._layout)\n",
        "    s[y, x] = 4\n",
        "    return s\n",
        "\n",
        "  # Environment: given an action as input, return [state, reward]\n",
        "  def step(self, action):\n",
        "    y, x = self._state\n",
        "    \n",
        "    # Next state\n",
        "    if action == 0:  # up\n",
        "      new_state = (y - 1, x)\n",
        "    elif action == 1:  # right\n",
        "      new_state = (y, x + 1)\n",
        "    elif action == 2:  # down\n",
        "      new_state = (y + 1, x)\n",
        "    elif action == 3:  # left\n",
        "      new_state = (y, x - 1)\n",
        "    else:\n",
        "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
        "\n",
        "    # Next reward\n",
        "    new_y, new_x = new_state\n",
        "    if self._layout[new_y, new_x] == -1:  # wall\n",
        "      reward = -5.\n",
        "      discount = 0.9\n",
        "      new_state = (y, x)\n",
        "    elif self._layout[new_y, new_x] == 0:  # empty cell\n",
        "      reward = -1\n",
        "      discount = 0.9\n",
        "    else:  # a goal\n",
        "      reward = self._layout[new_y, new_x]\n",
        "      discount = 0.\n",
        "      new_state = self._start_state\n",
        "    \n",
        "    # Add noise to the reward (?)\n",
        "    if self._noisy:\n",
        "      width = self._layout.shape[1]\n",
        "      reward += 2*np.random.normal(0, width - new_x + new_y)\n",
        "    \n",
        "    self._state = new_state\n",
        "\n",
        "    return reward, discount, self.get_obs()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBWgU589Du8u"
      },
      "source": [
        "class classicRLSolver:\n",
        "  def __init__(self, number_of_states, number_of_actions, initial_state, target_policy, behaviour_policy, double, step_size=0.1):\n",
        "    self._q = np.zeros((number_of_states, number_of_actions))\n",
        "    if double:\n",
        "      self._q2 = np.zeros((number_of_states, number_of_actions))\n",
        "    self._s = initial_state\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._step_size = step_size\n",
        "    self._behaviour_policy = behaviour_policy\n",
        "    self._target_policy = target_policy\n",
        "    self._double = double\n",
        "    self._last_action = 0\n",
        "    \n",
        "  @property\n",
        "  def q_values(self):\n",
        "    if self._double:\n",
        "      return (self._q + self._q2)/2\n",
        "    else:\n",
        "      return self._q\n",
        "\n",
        "  def step(self, r, g, s):\n",
        "    \"\"\"\n",
        "    Params: \n",
        "    r (int): reward value sampled from the environment. Used for the update\n",
        "    g (float): discount factor\n",
        "    s (int): next state (t+1) sampled. Bootstrap from s\n",
        "    Returns:\n",
        "    next_action (int): refer to Grid() class for action encoding\n",
        "    \"\"\"\n",
        "\n",
        "    if self._double:\n",
        "      values = [self._q, self._q2]\n",
        "      random.shuffle(values)\n",
        "    else:\n",
        "      values = [self._q, self._q]\n",
        "    \n",
        "    a_prime = self._behaviour_policy(values[0][s])\n",
        "\n",
        "    # Next target action, used for the update\n",
        "    target_a_prime = self._target_policy(values[0][s], a_prime)\n",
        "\n",
        "    values[0][self._s, self._last_action] += self._step_size*\\\n",
        "      (r + g*np.dot(values[1][s], target_a_prime) - values[0][self._s, self._last_action])\n",
        "\n",
        "    # Next action: behaviour - in double-q, sample using updated value\n",
        "    self._s = s\n",
        "    self._last_action = a_prime\n",
        "\n",
        "    return a_prime"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "dv2xQMM-GOST",
        "outputId": "6769bd3c-5eb8-4bbb-99ad-58dbef36455c"
      },
      "source": [
        "grid = Grid()\n",
        "\n",
        "def target_policy(q, a):\n",
        "  \"\"\"\n",
        "  :param q: action-value vector for a given state \n",
        "  \"\"\"\n",
        "  # One hot vector with one for the most valued action\n",
        "  return np.eye(len(q))[np.argmax(q)]\n",
        "\n",
        "def behaviour_policy(q):\n",
        "  return epsilon_greedy(q, 0.1)\n",
        "\n",
        "agent = classicRLSolver(grid._layout.size, 4, grid.get_obs(),\n",
        "                 target_policy, behaviour_policy, double=False)\n",
        "run_experiment(grid, agent, int(1e5))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdcAAAHOCAYAAADKXUOSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3df7CkVZ3f8c937jgL6rDM8HMYkeWXZGAZXBxAKVZERMdEwLDJFsRSSEhSlpqNa6JB3SLEYpHdZasoV+I6m7ADW4moGyPDgssiKKghwzgrIogERBCQAa6IjDsOd+7tT/7o50pzf/czp/s89znvV9Utbj/dp/vc8fj99DnPebrDtgAAQDpLcncAAIC2IVwBAEiMcAUAIDHCFQCAxAhXAAASI1wBAEiMcAUAIDHCFQCAxAhXYDdFxCcj4oOZXvuuiDgmx2sDmB3hCuyGiNhP0nskfTZTF66Q9IlMr51URFwdEU9HxL09x1ZGxC0R8WD13xU5+wgsFOEK7J4LJN1k+5eZXn+TpNMi4sBMr5/SRknrpxy7SNKtto+UdGt1G2g8whWYR0QsiYiPVrOqZyLifRExVs1a3y7p9imPPywiboyI0Yh4PiJumef5HRFH9NzeGBGX9tx+pHr970fEzyLiLyNiD0myvVPSVklvS/k352D7DknPTjl8tqRrqt+vkfTOoXYKqIlwBeZ3saR3SFor6Qh1l4Gftf2MpGMlPTDl8ddKuknSAZL2l3RJgj68S90APVzSayT9Qc9990s6LsFrNNEBtp+sft+m7r8p0HhLc3cAaLJqdvohSWttb6uO3SjpjdVD9pa0fUqzwyWNSBqpZpbfStCVT9t+rHr9P5T0Z3oxYLdLWpXgNZJYv369R0dHpx3funXrfZJ29hzaYHvDQp/XtiOCr/HCokC4AnM7XdL9th/pObaPpO9Vv/9M0vIpbd4l6eOSLo6I6yV92PbU5c5+Pdbz+6OSDuq5vVzSc7v5/MmMjo5qy5a7ph1fsmRkp+11fT7dUxGxyvaTEbFK0tNJOgkMGMvCwNz2lfTM5I2IWCrpLEmTO1rvUXeZ9lds32b7dElHq7tce8E8r7FD0st7bs+0Oengnt9fLeknPbfXSPruPK8xRJbUmeGnlk2Szq9+P1/S9bvbO2AYCFdgbj+QdHJEHFpdBvIZSYfqxZnrTZJOnXxwRJwTEUdGRKg7o1wh6e7qvo0RsXGG17hb0r+IiJGIWN/7fD3eHxGvioiV6s6KP1895x6SXidpzk1Tw2Z3pv3MJyI+J+lOSUdFxOMRcaGkyyWdEREPSnpLdRtoPJaFgTnY/mpEfFHdmeE2SZ9Wdxp2X/WQayXdHRF7VpfjnFI9Zi9JT0i63PZt1WMPlnTdDC/z79XdCft+SV+ufqb6n5L+Tt3l4OslTe4mPlPS123/ZIY22SwkTKe38Xmz3HX67vUGGL6w2R8ALFREvFXSVdV1l5PHLpP0tO0r52i3TN2AXmt7V5+v+Yikf237qzPct1nShbbvndYwk3Xrjvfmzd+cdnzp0ldsrXHOFViUmLkC/VmjF5eEJUm2PzZfI9tjVdukbJ+U+jl3l11v5gq0CeEK9GeNXtzMhBlZ9kTuTgBZEa5AH2y/N8Nr/sawX3N3MXNF6QhXAIlZEjNXlI1wBZAcM1eUjnDFojISYQZtM4xLmrBj+j0mXFE86hQWlaWa+eOLMHzb5ryXcEXZCFcASdnMXAHCFUByXIqD0hGuABJj5goQrgCSY+aK0hGuABLjOleAcAWQHMvCKB3hCiApdgsDhCuAgWBZGGUjXAEkxswVIFwBJMduYZSOcAWQGDNXgHAFkJQt2eO5uwFkRbgCSMzig/tROsIVQHKcc0XpCFcAiZlwRfEIVwADwLIwyka4AkiMmStAuAJIjHAFCFcASXUvxWFZGGUjXAGkZ+fuAZAV4QogMaszwcwVZSNcAaRlSR1mrigb4QogOROuKBzhCiA5c84VhSNcAaTFsjBAuAJIyzLLwige4QrM48R162q1O+GII2q1W7N6da1212/ZUqudJN1yxx21205jzrkChCuA5DjnitIRrgCS8wThirIRrgDSYlkYIFwBpGZ2C6N4hCuApMzMFSBcAaRHuKJ0hCuA5AhXlI5wBZAWn9AEEK4AUuMTmgDCFUBalsz3uaJwhCuApCzOuQKEK4C0zLIwQLgCSI5wRekIV2Aeh+6/f612db/dZtUxB9Vq99Fz/02tdpJ0ywkJvxVHhCtAuAJIi0txAMIVQHrMXFE6whVAUmZDE0C4AkjPHa5zRdkIVwBpWfI4M1eUjXAFkBzLwigd4QogLc65AoQrgAHgnCsKR7gCSMqWPMHMFWUjXAEkx7IwSke4AkjLZuaK4hGuAJJj5orSEa4A0rLkcTY0oWyEKzCPz990U612p6xZU6vdr+27Z612bz7h3bXapcaXpQOEK4DUbHmCmSvKRrgCSI+ZKwpHuAJIi+tcAcIVQGKEK0C4AkiNc64A4QogKZvdwgDhCiA5loVROsIVQFq2vItlYZSNcAWQlsU5VxSPcAWQHOdcUTrCFUBSljRhwhVlI1wBpGWr02FZGGUjXAEkZUkdZq4oHOEKDMiSiFrtlv/Gilrt3n7aabXaSdJXvva12m1nwswVpSNcASRlm5krike4AkhugpkrCke4AkjKYlkYIFwBpGVzKQ6KR7gCSIqZK0C4AhgANjShdIQrgKRss6EJxSNcASTHzBWlI1wBJMc5V5SOcAWQlG2NE64oHOEKICmLD5EACFcAybEsjNIRrgCSMh8iARCuANJj5orSEa7AgLz/iivqNazZrCk45woQrgAS40MkAMIVwACwLIzSEa4AkmLmChCuABKzpF0TE7m7AWRFuAJIi5krQLgCSIvdwgDhCiA1Zq4A4QogLWauAOEKIDHCFSBcAaTGV84BhCuAtCxpgktxUDjCFUBStjU2Pp67G0BWhCuApPiEJoBwxSIzJo3+WHo0dz8gSTpkpoOWNM6yMApHuGJRsb1f7j5gHjbhiuIRrgCS4rOFAcIVQGJm5goQrgDSss3MFcUjXAEkZUm7uBQHhSNcASTV6XT0y7Gx3N0AslqSuwMA2sW2xnbtmvazEBGxPiIeiIiHIuKiAXcVGBhmrgCS6th6YYFh2isiRiRdJekMSY9L2hIRm2x/P3EXgYEjXAEk5U5HO3fsqNP0REkP2X5YkiLiOklnSyJcsegQrgCS6nQ62rF9e52mqyU91nP7cUknJekUMGSEK4CkXrBv/uHY2L4z3LVHRHy75/YG2xuG1S9gmAhXAEnZXl+z6ROSDu65/arqGLDosFsYQFNskXRkRBwaEcsknStpU+Y+AbUwcwXQCLbHI+IDkm6WNCLpatv3Ze4WUEvYzt0HAABahWVhAAASI1wBAEiMcAUAIDHCFQCAxAhXAAASI1wBAEiMcAUAIDHCFQCAxAhXAAASI1wBAEiMcAUAIDHCFQCAxAhXAAASI1wBAEiMcAUAIDHCFQCAxAhXAAASI1wBAEiMcAUAIDHCFQCAxAhXAAASI1wBAEiMcAUAIDHCFQCAxAhXAAASI1wBAEhsUYZrRHwyIj64G+0fiYi3LPCxR0XE3RGxPSJ+b5bH3BURx9TtD8qx0LHLmMJUu1v3ZnnO+yLiTQt87ILrZvX4osfwogvXiNhP0nskfXZIL/kRSV+zvdz2p2YZYFdI+sSQ+jMQEXF1RDwdEff2HFsZEbdExIPVf1fk7ONi1+fYXfRjql+MwdkNqu7ZPsb211M81wy1cVGO4VTjcNGFq6QLJN1k+5dDer1DJN03z2M2STotIg4cQn8GZaOk9VOOXSTpVttHSrq1uo36LtDCx24bxlS/NooxOJsLlLDuRcTSFM8zj8U6hjcqwThsZLhGxJKI+Gj17uGZiHhfRIxV797eLun2KY8/LCJujIjRiHg+Im7p8/UOioj/Vb3WjyaXfyPiNkmnSfp0RPwiIj4n6dWSbqhuf0SSbO+UtFXS23b/r8/D9h2Snp1y+GxJ11S/XyPpnUPt1CITEXtFhCNi355jR0fEUxHx65oyducat20YU/0qfQwOuu5VM8v/FBH3SPqHiFjaO9uMiOMj4jvVKbAvRsTnI+LSKU/z2oi4JyJ+Xt2/R9X2rzSlNi7WMZxqHDYyXCVdLOkdktZKOkLd5ZBnbT8j6VhJD0x5/LWSbpJ0gKT9JV2y0BeKiCWSbpD0XUmrJZ0u6YMR8Tbbb5b0DUkfsP1K2+dJ+rGkM6vbf9zzVPdLOq7fP7ThDrD9ZPX7NnX/fTEL289LekzS0T2H/1DSH9n+uaaP3fnGbRvHVL9KGoPDqHvnSfonkva2PT55MCKWSfrf6s7aVkr6nKR/OkP731V3Vndo1c8LJMn2uzVzbWzLGO57HA5jaaAv1bu0D0laa3tbdexGSW+sHrK3pO1Tmh0uaUTSSPVu6Vt9vOQJkvazPXlu4OGI+AtJ50q6uY/n2S5pVR+PH4j169d7dHR02vGtW7feJ2lnz6ENtjcs9HltOyKcoIttd6+kNZLuiIiTJB2v7liSpo/d+cZtI8ZUv9avP9ijozunHd+6dZQxOIsh1r1P2X5shuOvVzcPPmXbkr4UEXfN0v4nVf9ukPTaeV4vyxierQ5Kw6uFjQtXdWeO99t+pOfYPpK+V/3+M0nLp7R5l6SPS7o4Iq6X9GHbU6f1szlE0kER8VzPsRF1Z6z9WC7puXkfNWCjo6PasmX6/yeWLBnZaXtdn0/3VESssv1kRKyS9HSSTrbbvXpx5nqZpEtsv1Ddnjp25xu3jRhT/Rod3am7tpwz7fjIkg2MwdkNq+7NFKySdJCkJ6pgneux23p+31G1m0uWMTxbHZSGVwubuCy8r6RnJm9UJ97PUrdoSdI9kl7T28D2bbZPV7eoHadqqWKBHpP0I9t79/wst/2PZ3n8bO9Y1qi7tJyZJXVm+Kllk6Tzq9/Pl3T97vauAPdKOro6j3Wgukt3k14ydhcwbhsypvpjSR1P/6mplDE4rLo32/8ST0paHRHRc+zgBfV87ufONIZnq4PDq4VNDNcfSDo5Ig6ttjt/Rt31/cl3cDdJOnXywRFxTkQcWQ2K5ZJWSLq7um9jRGyc5/XukrS9OtG/Z0SMRMRvRsQJszz+KUmH9R6oTuq/TlJfG6kGxe5M+5lPtVnrTklHRcTjEXGhpMslnRERD0p6S3Ubc5ucuV4m6eO2J3ru+9XYnWvcVvc3akz1q064Fj4Gh133prpT0oSkD1Qbnc6WdGKfz/GS2ph7DM9UB4dZCxu3LGz7qxHxRXXf7WyT9Gl1325MXg5zraS7I2LPalv6KdVj9pL0hKTLbd9WPfZgSdfN83oTEfEOSX8q6UeSfk3djQN/MEuTT0r6s4j4Y0mX2r5C0pmSvj55LiK3hQyg6W183ix3nb57vSnO99Wdsf7Y9pen3Persau5x63UsDHVl5oz1ZLH4LDr3gyvPxYR50j6b+rWuK9I+htJL8zZ8KVeUhslPaqMY7hOHey2SzMO46VL7M0TEW+VdFV1fdHkscskPW37yjnaLVN3oK61vWvAfdws6ULb98774AFbt+54b978zWnHly59xdYa5xmQ2ELGbvW4xoypfh3/uv38zf87/ZzrK5ZtYAwuUBPqXjUG/9z2X+5G+yxjeLY6KA2vFjZu5jqDNXpxaUSSZPtj8zWyPVa1HTjbJw3jdRbCrv+ODYO3kLFbPa4xY6qO3TjHiq6h172IOFXdVbtRdTdLrZX0t3Weq+pLtjHchDq4WMJ10b17z8fZBxXKNrmhCbslR907StIXJL1C0sOS/lnPtZ2LTP462Phwtf3e3H1YbF66hwYYvgnCdbfkqHvVtZ4Lvt6z6XLXwcaHK/pldTf9AXl49y69ARLIXwcJ1xbKvRwCdBiCyCx3HewrXEciTBo3w7ikCTum35P/XMMgxcv3sPae+kE1yOK57fKOndPGYNvPuVIHm6PJdbCvMbJU3Qv4kN+2Oe9tb7hq7+XSv51+mQcy2PClWe9q8QikDjZIk+sgb8Baxs7/jg1o88wVzdeEOki4tlDuXXIoGxua0AS56yDh2jr537EBhCvyyl8HCddWIlyRT9s3NGGxIFyRlGWP5+4ESmZpgvd3yCp/HSRcWyj3cgjKxswVTZC7DhKuLdOEXXIA4YqcmlAHCddWYrcw8iJckR+7hZFU/ndsKBvLwsgvfx0kXFso9/VdKBzXuaIBctdBwrV18r9jQ9mYuSK//HWwteG6z6pVtdqtX7euVrt1hx9eq50k/f6VV9ZuOzNmrlPt+bL6bQ/59XrtDl1Rr93hNdvdP1qvnSTd+nD9tjMhXNP6zWOPrdXuvW99a612f3r99bXa/eihh2q1Gwxmrkiou0uOcEU+Fte5Iq8m1EHCtYVyDyoUjnOuaIDcdZBwbZ3879gAJq7IK38dJFxbidKGfNjQhGZgQxOSyv+ODSBckVf+Oki4tk7+QYWyMXNFfvnrIOHaMnb+D6wGCFfk1IQ6SLi2kalsyIjdwmiCzHWQcG0dq8NFhsiI61yRX/46SLi2DSe80AAMQWTVgDpIuLaQqWzIqAF1DcheBwnXFjLnXJET51zRALnrIOHaNkwbkBlDENk1YBASri1jOftyCMAQRE5NqIOND9cTa34F3AlHHFGr3ZrVq2u1W7nPXrXaJef85xqaqO7XxknSESvrtTtyn3rt1uxbr90/P6ZeO4mvnBuG9597bu22devSqmMOqtXuv5/7n2u1+8Kf3FCrnST9+Re+ULvtNA2og40PV/Qv97kGlK0BK3JA9jpIuLYRlQ05saEJTcDMFUlZ6kxQ2ZCPJTEEkVUD6iDh2jpMG5AfQxB55a+DhGvLuAEn8lE2zrkitybUQcK1hXIPKoAhiNxy10HCtYVyDyqUzflX5IDsdZBwbRvW5NAADEFk1YA6SLi2Tv5PJgEYgsgrfx0kXNumASfyUbYGTBpQugbUQcK1ZSzJfFM1cjJflo68mlAHCde2cf7lEJSNmSuya0AdJFxbKPegAhiCyC13HWx8uI5E1Go37G+RWHHs/rXaSZI+Vr/pTHIPqiZatbx+28NrfitO3W+3WXtAvXbHHVivXWrMXGd2xIH1/wcadl1acdjhtdqdesKxtdpJib8VR/nrYOPDFX2isqEBGILIqgF1kHBtodzv2FA4PkQCDZC7DhKuLeMGnMhH2RowaUDhmlAHCdcWcofrIJAX4YrcctdBwrVtLHmcyoZ8+D5XZNeAOki4tlDu5RCAIYjcctdBwrVtGnCuAWXjW3GQXQPqIOHaRpxzRWaEK7LjnCtSsiVzwguZEa7IqQl1kHBtodzLISgbl+KgCXLXQcK1bezs79gAwhVZNaAOEq4tlPsdG8rGhiY0Qe46SLi2jfNfPA0QrsiqAXWw8eF655YttdqdsmZNrXYHv/zVtdq9+YR312qXmpX/4ukm+vsn67c966h67V6zT712x9b8Vpw9L63XLjWLL0ufyV/feWfttp849/ha7ep+u83KlW+o1e68D59cq11qTaiDjQ9X9MmWqWzIjJkrsmpAHSRc24jKhpw454om4JwrkmrA9V0oG5fiILsG1EHCtW0aMKgAwhVZNaAOEq6tk/9cA8rGzBX55a+DhGvL2Pmv7wIYgsipCXWQcG2h3MshKBwbmtAAuesg4do2tryLZWHkw5elI7sG1EHCtW2s7OcaAGauyKoBdZBwbaHc5xpQNjY0oQly10HCtWW6S3JUNuRFuCKnJtRBwrVtbHX44H5kxLfiILsG1EHCtWW6S3JUNuRFuCKnJtTB1obrn1x77VDbNUnud2xN9POd9dv+/t/Wa/f2I+q1e2e9L3TSAa+s106SnvpF/bYzIVyn+9bmzbXbnv76+m1LlbsOtjZcS2U7+zs2lI0NTcitCXWQcG2h3IMKIFyRW+46SLi2jCVNTEzk7gYKZvNl6cirCXWQcG0bO/sWdICZK7JqQB0kXFvGyn8iH2XjnCtya0IdJFxbKPe5BoBwRW656yDh2jK2NcHMFRnxIRLIrQl1kHBtodzv2ADCFbnlroOEawvlPtcAEK7ILXcdJFxbpgnLISgbG5qQWxPqIOHaMpY0TrgiJ/Nl6cirCXWQcG2h3MshKBszVzRB7jpIuLaMG3DxNEC4Iqcm1EHCtYVyv2ND2Zi5ogly10HCtWUsZT+Rj66vPDTcdk1CuCKnJtRBwrVlmrBLDoXjQySQWRPqIOHaQrmXQ1A2loXRBLnrIOHaMk14xwYQrsipCXWQcG0ZS9rF97kio+75rty9QMmaUAcJ17ZpwDs2FI5zrsitAXWQcG2ZJuySQ9k454rcmlAHCde2acA7NoBwRVYNqIOEa8s04R0bQLgipybUQcK1ZZowqFA2loWRWxPqIOHaNnb2b4NA4djQhNwaUAcJ15axpAkuxUFGzFyRWxPqIOHaMm7AOzaAcEVOTaiDhGvL2Nau8fHc3UDBLL4sHXk1oQ72Fa5j0uiPpUcH1Rn05ZCZDlrSeJuXhZ8cHdV/2cAYbIYZx6DU7pkrdbBRGlsH+wpX2/sNqiNIxM4+qAaJMbgItHxDE2NwEWhAHWRZuGWa8JmaKBsbmpBbE+og4doybsA7NoBwRU5NqIOEa8vYzv6ODWVj5orcmlAHCdeWsZR9lxxAuCKnJtRBwrVlOrbGCFfk1PINTWi+JtTBJVlfHcm509EvX3hh2s9CRMT6iHggIh6KiIsG3FW01OSXpU/9WQjGIFKYrQ4upBamGoPMXFumY+uFXbv6bhcRI5KuknSGpMclbYmITba/n7iLKECdmStjEKk0oQ4Sri3jTkc7d+yo0/RESQ/ZfliSIuI6SWdLorChL7uxoYkxiCSaUAcJ15bpdDrasX17naarJT3Wc/txSScl6RTKUv+cK2MQSTShDhKuLfOCffMPx8b2neGuPSLi2z23N9jeMKx+oSBPjt7sSzYwBpHNHHVQGtI4JFxbxvb6mk2fkHRwz+1XVceAvjAGkVsTxiC7hTFpi6QjI+LQiFgm6VxJmzL3CWVhDCK3ZGOQmSskSbbHI+IDkm6WNCLpatv3Ze4WCsIYRG4px2DYXO0NAEBKLAsDAJAY4QoAQGKEKwAAiRGuAAAkRrgCAJAY4QoAQGKEKwAAiRGuAAAkRrgCAJAY4QoAQGKEKwAAiRGuAAAkRrgCAJAY4QoAQGKEKwAAiRGuAAAkRrgCAJAY4QoAQGKEKwAAiRGuAAAkRrgCAJAY4QoAQGKEKwAAiRGuAAAkRrgCAJAY4QoAQGKEKzCHiPhkRHww4fNtjIhLUz3fAl/zrog4ZpivCZSOcAVmERH7SXqPpM/m7stuukLSJ3J3Yj4RcXVEPB0R9/YcWxkRt0TEg9V/V+TsI7BQhCswuwsk3WT7l7k7sps2STotIg7M3ZF5bJS0fsqxiyTdavtISbdWt4HGI1xRtIhYEhEfrWZMz0TE+yJirJq1vl3S7VMef1hE3BgRoxHxfETcMs/z/1ZE/H1EbI+Iz0vaY8r9ayLi6xHxXETcFxFnVcf/ZUTc0PO4ByPiiz23H4uI11a/PxIR/zEi7omIn0fE5yPiV69je6ekrZLeVvsfaghs3yHp2SmHz5Z0TfX7NZLeOdROATURrijdxZLeIWmtpCPUXQZ+1vYzko6V9MCUx18r6SZJB0jaX9Ilsz1xRCyT9GVJfyVppaQvSvqdnvtfJukGSX9XPde/k/Q/IuIodUP9t6vwP0jSMklvqNodJumVku7pebnfVXfWd2j1t1wwpTv3Szpunn+LJjrA9pPV79vU/XcHGm9p7g4AuVSz0w9JWmt7W3XsRklvrB6yt6TtU5odLmlE0kg1I/zWHC/xekkvk3SlbUv664j40JT7XynpctsdSbdFxN9IOs/2JRGxXdJrJb1G0s2SXhsR/0jdkP1G1WbSp2z/pPobbqja9douadXc/yJprF+/3qOjo9OOb9269T5JO3sObbC9YaHPa9sR4QRdBAaOcEXJTpd0v+1Heo7tI+l71e8/k7R8Spt3Sfq4pIsj4npJH7Y9dSlz0kGSnqiCddKjU+5/bEpIPippdfX77ZLepO6M+nZJz0k6Vd1wfclytbqzukk7qufutbxqP3Cjo6PasuWuaceXLBnZaXtdn0/3VESssv1kRKyS9HSSTgIDxrIwSravpGcmb0TEUklnSZrcrXqPurPGX7F9m+3TJR2t7jLrBXM8/5OSVkdE9Bx7dc/vP5F0cEQsmXL/E9Xvk+H629Xvt6sbrqdqerjOZ42k7/bZpiZL6szwU8smSedXv58v6frd7R0wDIQrSvYDSSdHxKHVJR6fUfec5eTM9SZ1g0ySFBHnRMSRVVgul7RC0t3VfRsjYuOU579T0rik34uIl0XEOZJO7Ll/s7qzzI9U979J0pmSrqvuv13SaZL2tP24pG+oe151H0nfWegfWW1uep2kOTdfpWR3pv3MJyI+p+6/2VER8XhEXCjpcklnRMSDkt5S3QYaj2VhFMv2V6sduN9Vd1n10+pOse6rHnKtpLsjYs/qcpxTqsfspe7s8nLbt1WPPVgvhuLk849VgfoXki5VN6y/NOX+MyX9V0kfrZ7zPbZ/UN3//yLiF+qGqmw/HxEPS3rG9kQff+qZkr4+eU52GBYSptPb+LxZ7jp993oDDF+89HQQUK6IeKukq6prKiePXSbpadtXztFumboBvdb2rsH3tD8RsVnShbbvnffBCaxbd7w3b/7mtONLl75ia41zrsCixMwVeNEavbgkLEmy/bH5Gtkeq9o2ku2Thvt69WauQJsQrsCL1ujFzUyozYQrike4AhXb783dh/YgXFE2whVAYpY9nrsTQFaEK4DkWBZG6QhXLCojEWbQNsO4pAk7pt/DOVeAOoVFZamkpn9vWim2zXkv4YqyEa4AkrKZuQKEK4Dk+vsAKaB9CFcAiTFzBQhXAANAuKJshCuAxLjOFSBcASTHsjBKR7gCSIrdwgDhCmAg2C2MshGuABJj5goQrgCS4zpXlI5wBZAYM1eAcEURjlu7tnbbU44+ula7ly9bVqvd6990XK12v/Ov/kOtdoPBzBVlI1wBJMVuYYBwBTAAfIgESke4AkjMbGhC8QhXAAPAsjDKRrgCSIyZK0C4AkiMcOoA8XQAAARySURBVAUIVwBJ2XxwP0C4AkjPzt0DICvCFUBy7hCuKBvhCiAtW55gWRhlI1wBJMfMFaUjXAEkZ865onCEK4C0LImZKwpHuAJIyjLLwige4QrMY83q1bXarTrmoFrtVhy7f6123/np/6nVTpJ+a5+Ta7edxpxzBQhXAMlxzhWlI1wBpMfMFYUjXAGkZakzQbiibIQrgMTMzBXFI1wBJGU2NAGEK4D0CFeUjnAFkBzhitIRrgDS4hOaAMIVQGp8QhNAuAJIiw1NAOEKIC2LcAUIVwBp8WXpAOEKID1mrigd4YoifPeee2q3XX7WWbXa1f12mxWHHV6r3cqVb6jVbhAIV5SOcAWQFpfiAIQrgPSYuaJ0hCuApGyucwUIVwDJucNuYZSNcAWQHDNXlI5wBZCWJY8Trigb4QogLc65AoQrgAHgnCsKR7gCSMqWPMHMFWUjXAEkx7IwSke4AkjLZuaK4hGuAJJj5orSEa4A0jIfIgEQrsA8zr/00lrt9vrMPrXafezcc2u1u+iqk2u1GwiWhVE4whVAUrbVGWfmirIRrgDS45wrCke4AkiL61wBwhVAYoQrQLgCSM3yBOdcUTbCFUBSNte5AoQrgORYFkbpCFcAaZllYYBwBZCWJe8iXFE2whVAcpxzRekIVwBJWdKECVeUjXAFkJatDh/cj8IRrgCSsqQOM1cUjnAFBuT5n/60VruLrroqcU+Gj5krSke4AkjKNjNXFI9wBZAc4YrSEa4AkrKkiYmJ3N0AsiJcAaRlcykOike4AkjKYkMTQLgCSI5zrigd4QogKduaYOaKwhGuAJJj5orSEa4AkuOcK0pHuAJIimVhgHAFkBifLQwQrgAGYJwPkUDhCFcASZkPkQAIVwDpsaEJpSNcASRliQ1NKB7hCiApdgsDhCuAAWBZGKUjXAEkxcwVIFwBDADhitIRrgCSsq1dXOeKwhGuAJJitzBAuAJIjXOuAOEKIC1mrgDhCiAxwhUgXAGkZmuccEXhCFcASVnSBLuFUTjCFUBSZuYKEK4A0mPmitIRrlhUxqTRH0uP5u4HJEmHzHSwY2tsfHzYfQEahXDFomJ7v9x9wDxsjTNzReEIVwBJWeLjD1E8whVAUmbmChCuANLig/sBwhVAYpa0iw1NKBzhCiApdgsD0pLcHQDQLra1a3x82s9CRMT6iHggIh6KiIsG3FVgYJi5Akiq0+noH3bu7LtdRIxIukrSGZIel7QlIjbZ/n7iLgIDR7gCSMqdjnbu2FGn6YmSHrL9sCRFxHWSzpZEuGLRIVwBJNXpdLRj+/Y6TVdLeqzn9uOSTkrSKWDICFcASb1g3/zDsbF9Z7hrj4j4ds/tDbY3DKtfwDARrgCSsr2+ZtMnJB3cc/tV1TFg0WG3MICm2CLpyIg4NCKWSTpX0qbMfQJqYeYKoBFsj0fEByTdLGlE0tW278vcLaCWsJ27DwAAtArLwgAAJEa4AgCQGOEKAEBihCsAAIkRrgAAJEa4AgCQGOEKAEBihCsAAIn9f9h6H5rAKTdPAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x576 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tWvaVO8Ganu"
      },
      "source": [
        "expert_q = q[1:-1, 1:-1, :]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4DxLRFcFCYQ"
      },
      "source": [
        "# IRL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E96ZGzb1vraH"
      },
      "source": [
        "def plot_trajectory(world, t):\n",
        "  plt.figure(figsize=(3, 3))\n",
        "  plt.imshow(world.layout > -1, interpolation=\"nearest\")     \n",
        "  ax = plt.gca()\n",
        "  ax.grid(0)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.title(\"The grid\")\n",
        "\n",
        "  start = t[0][0]\n",
        "  end = t[-1][0]\n",
        "  plt.text(\n",
        "      start[1], start[0], \n",
        "      r\"$\\mathbf{S}$\", ha='center', va='center')\n",
        "  plt.text(\n",
        "      end[1], end[0], \n",
        "      r\"$\\mathbf{G}$\", ha='center', va='center')\n",
        "\n",
        "  action_names = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
        "\n",
        "  for state, action in t[:-1]:\n",
        "    action_name = action_names[action]\n",
        "    plt.text(state[1], state[0], action_name, ha='center', va='center')\n",
        "\n",
        "\n",
        "def plot_greedy_policy(world, irlsolver):\n",
        "  plt.figure(figsize=(3, 3))\n",
        "  plt.imshow(world.layout > -1, interpolation=\"nearest\")     \n",
        "  ax = plt.gca()\n",
        "  ax.grid(0)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.title(\"The grid\")\n",
        "\n",
        "  start = world.point_to_state_coord(world.initial_state)\n",
        "  end = world.point_to_state_coord(world.terminal_state)\n",
        "  plt.text(\n",
        "      start[1], start[0], \n",
        "      r\"$\\mathbf{S}$\", ha='center', va='center')\n",
        "  plt.text(\n",
        "      end[1], end[0], \n",
        "      r\"$\\mathbf{G}$\", ha='center', va='center')\n",
        "\n",
        "  action_names = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
        "\n",
        "  for i in range(world.grid_size):\n",
        "    for j in range(world.grid_size):\n",
        "      s = (i, j)\n",
        "      q = []\n",
        "      for action in world.actions:\n",
        "        q.append((irlsolver.Q(world.state_coord_to_point(s), action)))\n",
        "      action_name = action_names[np.argmax(q)]\n",
        "      plt.text(i, j, action_name, ha='center', va='center')"
      ],
      "execution_count": 443,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYcfdDjhuvZG"
      },
      "source": [
        "class GridWorld:\n",
        "  def __init__(self, grid_size = 5):\n",
        "    \"\"\"\n",
        "    Attributes:\n",
        "      actions: actions available to the agent (one step in the grid)\n",
        "      grid_size: size of the side of teh squared grid\n",
        "      terminal_state: terminal state of the environment\n",
        "      p_initial: ... ?\n",
        "      state: state in terms of point ion range(0, geid_size**2)\n",
        "      state_coord: coordinates of the state in the grid\n",
        "    \"\"\"\n",
        "\n",
        "    self.actions = [0, 1, 2, 3] # [up, right, down, left]\n",
        "    self.grid_size = grid_size\n",
        "    self.layout = np.zeros((grid_size, grid_size))\n",
        "    self.terminal_state = self.state_coord_to_point((3, 4))\n",
        "    self.p_initial = np.zeros(self.layout.shape)*(1/self.grid_size**2) # must be computed form trajectories\n",
        "    self.gamma = 0.9\n",
        "\n",
        "    self.reset()\n",
        "\n",
        "  def state_coord_to_point(self, state_coord):\n",
        "    return state_coord[0]*self.grid_size + state_coord[1]\n",
        "\n",
        "  # TODO: test\n",
        "  def point_to_state_coord(self, point):\n",
        "\n",
        "    row = point // self.grid_size\n",
        "    col = point % self.grid_size\n",
        "    return row, col\n",
        "\n",
        "  #TODO: test\n",
        "  def simulate_step(self, state, action):\n",
        "    state_coord = self.point_to_state_coord(state)\n",
        "    next_s_coord = None\n",
        "\n",
        "    if action == 0:\n",
        "      if state_coord[0] == 0:\n",
        "        next_s_coord = state_coord\n",
        "      else:\n",
        "        next_s_coord = (state_coord[0]-1, state_coord[1])\n",
        "\n",
        "    elif action == 1:\n",
        "      if state_coord[1] == (self.grid_size-1):\n",
        "        next_s_coord = state_coord\n",
        "      else:\n",
        "        next_s_coord = (state_coord[0], state_coord[1]+1)\n",
        "      \n",
        "    elif action == 2:\n",
        "      if state_coord[0] == (self.grid_size-1):\n",
        "        next_s_coord = state_coord\n",
        "      else:\n",
        "        next_s_coord = (state_coord[0]+1, state_coord[1])\n",
        "\n",
        "    elif action == 3:\n",
        "      if state_coord[1] == 0:\n",
        "        next_s_coord = state_coord\n",
        "      else:\n",
        "        next_s_coord = (state_coord[0], state_coord[1]-1)\n",
        "\n",
        "    if self.state_coord_to_point(next_s_coord) == self.terminal_state:\n",
        "      # return self.state_coord_to_point(self.initial_state), 0\n",
        "      return self.state_coord_to_point(next_s_coord), 0\n",
        "\n",
        "    return self.state_coord_to_point(next_s_coord), self.gamma\n",
        "\n",
        "  def step(self, action):\n",
        "    next_state, gamma = self.simulate_step(self.state, action)\n",
        "    self.state = next_state\n",
        "\n",
        "    return next_state \n",
        "\n",
        "  # Initialize the gridworld\n",
        "  def reset(self):\n",
        "    initial_state = random.choice(range(self.grid_size**2))\n",
        "    while (self.point_to_state_coord(initial_state)) == self.terminal_state:\n",
        "      initial_state = random.choice(range(self.grid_size**2))\n",
        "    self.state = initial_state\n",
        "    self.initial_state = initial_state"
      ],
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImrBgjMT8YNd"
      },
      "source": [
        "class Trajectories:\n",
        "  def __init__(self, num_samples, world, expert_q):\n",
        "    self.world = world\n",
        "    self.num_samples = num_samples\n",
        "    self.trajectories = []\n",
        "    self.expert_q = expert_q\n",
        "\n",
        "    self.s = self.world.state # point\n",
        "\n",
        "  def sample_trajectories(self):\n",
        "    \n",
        "    terminal_state = self.world.terminal_state # point\n",
        "    for _ in range(self.num_samples):\n",
        "      self.world.reset()\n",
        "      state = self.world.state\n",
        "      trajectory = []\n",
        "      while state != terminal_state:\n",
        "        state_coord = self.world.point_to_state_coord(state)\n",
        "        row, col = state_coord\n",
        "        action = np.argmax(self.expert_q[row, col])\n",
        "        trajectory.append((state_coord, action))\n",
        "        state = self.world.step(action)\n",
        "\n",
        "      # Do not store the terminal state, since it is unique\n",
        "\n",
        "      trajectory.append((self.world.point_to_state_coord(state), None))\n",
        "      self.trajectories.append(trajectory)"
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_DeRVGZxKJC"
      },
      "source": [
        "class MaxCausalEntropy:\n",
        "  # We'll consider the case of a deterministic policy\n",
        "  def __init__(self, trajectories, world, lr, decay):\n",
        "    \"\"\"\n",
        "    Attributes:\n",
        "      trajectories: list of the sampeld trajectories\n",
        "      lr: learning rate for SGA\n",
        "      decay: weight decay for SGA\n",
        "      world: The environment modeled as GridWorld\n",
        "      features: feature to represrent each state. Simply encoded as coordinates in the grid\n",
        "      theta: learnable parameters, one vector for each possible state. Same dimensionality of features\n",
        "      V: (n_states x n_states) matrix with value associated to each state\n",
        "      D: (n_states x n_states) matrix with visitation frequencies\n",
        "      gamma: discount (Forse dev'essere ritornato da simulate_step, decido poi)\n",
        "    \"\"\"\n",
        "    self.trajectories = trajectories\n",
        "    self.lr = lr\n",
        "    self.decay = decay\n",
        "    self.world = world\n",
        "\n",
        "    self.n_states = self.world.grid_size**2\n",
        "    self.n_actions = len(self.world.actions)\n",
        "    self.features = np.zeros((self.n_states, 2))\n",
        "    for s in range(self.n_states):\n",
        "      i, j = self.world.point_to_state_coord(s)\n",
        "      self.features[s, 0] = i\n",
        "      self.features[s, 1] = j\n",
        "\n",
        "    self.theta = np.ones((self.n_states, 2))/self.n_states # np.matmul(self.theta[i, j], self.features[i, j]) = R(s=[i,j])\n",
        "    self.V = np.zeros((self.n_states))\n",
        "    self.D = np.zeros((self.n_states))\n",
        "    self.p_action = None\n",
        "\n",
        "\n",
        "  ######################## HELPER FUNCITONS ########################\n",
        "\n",
        "  def R(self, state):\n",
        "    \"\"\"\n",
        "    Return reward associated to given state (point)\n",
        "    \"\"\"\n",
        "    return np.matmul(self.theta[state], self.features[state])\n",
        "\n",
        "  # TODO Verify\n",
        "  def Q(self, state, action):\n",
        "    \"\"\"\n",
        "    Return action value associated to given state (point)\n",
        "    Maximize over action to get Q(S, A)\n",
        "    \"\"\"\n",
        "    next_s, gamma = self.world.simulate_step(state, action)\n",
        "    return self.R(state) + gamma * self.V[next_s]\n",
        "\n",
        "  def policy(self, state, action):\n",
        "    return np.exp(self.Q(state, action) - self.V[state])\n",
        "    \n",
        "  # f_b: the feature vectpr representing the expert policy\n",
        "  def feature_exp_from_trajectories(self):\n",
        "    _, features_dim = self.features.shape\n",
        "    f_exp = np.zeros((features_dim))\n",
        "\n",
        "    for trajectory in self.trajectories:\n",
        "      for step in trajectory:\n",
        "        state, _ = step\n",
        "        f_exp += self.features[self.world.state_coord_to_point(state)] # TODO: check if it's point or coord\n",
        "\n",
        "    return f_exp / len(self.trajectories)\n",
        "\n",
        "  def initial_probabilities_from_trajectories(self, n_states, trajectories):\n",
        "    p = np.zeros(self.world.layout.shape)\n",
        "\n",
        "    for t in trajectories: # for each trajectory\n",
        "      p[t[0][0][0], t[0][0][1]] += 1     # increment starting state\n",
        "\n",
        "    return p / len(trajectories)    \n",
        "\n",
        "  def softmax(self, x, y):\n",
        "    return np.log(np.exp(x) + np.exp(y))   \n",
        "\n",
        "\n",
        "  ######################## CORE FUNCTIONS ########################\n",
        "\n",
        "  def pseudo_gpi(self):\n",
        "    \"\"\"\n",
        "    Algorithm to update the value function\n",
        "    \"\"\"\n",
        "    eps = 1e-3\n",
        "    self.V = np.ones((self.n_states))*np.finfo(np.float32).min\n",
        "\n",
        "    delta = 2*eps\n",
        "    while delta > eps:\n",
        "      delta = 0\n",
        "      V_prime = np.ones((self.n_states))*np.finfo(np.float32).min\n",
        "      V_prime[self.world.terminal_state] = 0\n",
        "\n",
        "      # For each state and action, update the value function\n",
        "      for s in range(self.n_states):\n",
        "        for a in range(self.n_actions):\n",
        "          V_prime[s] = self.softmax(V_prime[s], self.Q(s, a)) # Il secondo termine è corretto? Dovrebbe esser Q(S, A)...\n",
        "\n",
        "      for s in range(self.n_states):\n",
        "        delta = max(np.abs(self.V[s] - V_prime[s]), delta)\n",
        "        self.V[s] = V_prime[s]\n",
        "\n",
        "\n",
        "  def expected_svf(self):\n",
        "    eps = 1e-5\n",
        "    self.D = np.zeros((self.n_states))\n",
        "\n",
        "    # Backward Pass\n",
        "    # 1. initialize at terminal states\n",
        "    zs = np.zeros(self.n_states)                             # zs: state partition function\n",
        "    zs[self.world.terminal_state] = 1.0\n",
        "\n",
        "    # 2. perform backward pass\n",
        "    for _ in range(2 * self.n_states):                       # longest trajectory: n_states\n",
        "        # reset action values to zero\n",
        "        za = np.zeros((self.n_states, self.n_actions))            # za: action partition function\n",
        "\n",
        "        # for each state-action pair\n",
        "        for s_from, a in product(range(self.n_states), range(self.n_actions)):\n",
        "\n",
        "            # sum over s_to\n",
        "            s_to, _ = self.world.simulate_step(s_from, a)\n",
        "            za[s_from, a] += np.exp(self.R(s_from)) * zs[s_to]\n",
        "        \n",
        "        # sum over all actions\n",
        "        zs = za.sum(axis=1)\n",
        "\n",
        "    # 3. compute local action probabilities\n",
        "    self.p_action = za / zs[:, None]\n",
        "\n",
        "    nonterminal = set(range(self.n_states)) - set([self.world.terminal_state])  # nonterminal states\n",
        "\n",
        "    delta = 2*eps\n",
        "    while delta > eps:\n",
        "      delta = 0\n",
        "      D_prime = np.reshape(\n",
        "          self.initial_probabilities_from_trajectories(self.n_states, self.trajectories),\n",
        "          (self.n_states)\n",
        "      )\n",
        "\n",
        "      # For each state and action, update the value function\n",
        "      for s in nonterminal:\n",
        "        for a in range(len(self.world.actions)):\n",
        "          next_s, gamma = self.world.simulate_step(s, a)\n",
        "          D_prime[next_s] += self.D[s]*self.p_action[s,a]\n",
        "\n",
        "      for s in range(self.n_states):\n",
        "        delta = max(np.abs(self.D[s] - D_prime[s]), delta)\n",
        "        self.D[s] = D_prime[s]\n",
        "      # print(delta)\n",
        "\n",
        "    # return self.D[s]\n",
        "\n",
        "  def compute_expected_svf(self):\n",
        "\n",
        "    nonterminal = set(range(self.n_states)) - set([self.world.terminal_state])  # nonterminal states\n",
        "    \n",
        "    # Backward Pass\n",
        "    # 1. initialize at terminal states\n",
        "    zs = np.zeros(self.n_states)                             # zs: state partition function\n",
        "    zs[self.world.terminal_state] = 1.0\n",
        "\n",
        "    # 2. perform backward pass\n",
        "    for _ in range(2 * self.n_states):                       # longest trajectory: n_states\n",
        "        # reset action values to zero\n",
        "        za = np.zeros((self.n_states, self.n_actions))            # za: action partition function\n",
        "\n",
        "        # for each state-action pair\n",
        "        for s_from, a in product(range(self.n_states), range(self.n_actions)):\n",
        "\n",
        "            # sum over s_to\n",
        "            s_to, _ = self.world.simulate_step(s_from, a)\n",
        "            za[s_from, a] += np.exp(self.R(s_from)) * zs[s_to]\n",
        "        \n",
        "        # sum over all actions\n",
        "        zs = za.sum(axis=1)\n",
        "\n",
        "    # 3. compute local action probabilities\n",
        "    self.p_action = za / zs[:, None]\n",
        "\n",
        "    # Forward Pass\n",
        "    # 4. initialize with starting probability\n",
        "    d = np.zeros((self.n_states, 2 * self.n_states))              # d: state-visitation frequencies\n",
        "    d[:, 0] = np.reshape(\n",
        "                  self.initial_probabilities_from_trajectories(self.n_states, self.trajectories),\n",
        "                  (self.n_states)\n",
        "              )\n",
        "\n",
        "    # 5. iterate for N steps\n",
        "    for t in range(1, 2 * self.n_states):                    # longest trajectory: n_states\n",
        "        delta = 0\n",
        "        # sum over nonterminal state-action pairs\n",
        "        for s_from, a in product(nonterminal, range(self.n_actions)):\n",
        "            s_to, _ = self.world.simulate_step(s_from, a)\n",
        "            delta = max((d[s_to, t] + d[s_from, t-1] * self.p_action[s_from, a] - d[s_from, t-1]), delta)\n",
        "            d[s_to, t] += d[s_from, t-1] * self.p_action[s_from, a] \n",
        "        # print(delta)\n",
        "    # 6. sum-up frequencies\n",
        "    return d.sum(axis=1)\n",
        "\n",
        "  def sga(self):\n",
        "    eps = 1e-4\n",
        "    delta = 2*eps\n",
        "    decay = self.decay\n",
        "    f_exp_from_t  = self.feature_exp_from_trajectories()\n",
        "    i = 0\n",
        "    while i < 100:\n",
        "      i+=1\n",
        "      delta = 0\n",
        "      for state in range(self.n_states):\n",
        "        for action in self.world.actions:\n",
        "          gradient = self.features[state]*self.D[state]*self.p_action[state, action] # Not sure this is right..., non c'è traccia delle azioni\n",
        "          update = np.exp(-(self.lr/decay)*(f_exp_from_t - gradient))\n",
        "          # delta = max(np.abs(self.theta[state] - self.theta[state]*update), delta)\n",
        "          self.theta[state] *= update\n",
        "          decay += 1\n",
        "\n",
        "    # return self.theta\n",
        "\n",
        "\n",
        "  ######################## ALGORITHM ########################\n",
        "\n",
        "  def maxirl(self):\n",
        "    eps = 1e-5\n",
        "    delta = 2*eps\n",
        "    i = 0\n",
        "    while i < 100:\n",
        "      self.pseudo_gpi()\n",
        "      self.D = self.compute_expected_svf()\n",
        "      # self.expected_svf()\n",
        "      self.sga()\n",
        "      i+=1\n",
        "      \n",
        "    return self.theta"
      ],
      "execution_count": 461,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdjeHQPJNSn5"
      },
      "source": [
        "# Experiements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8foWEeNNVZk"
      },
      "source": [
        "LR = 1\n",
        "DECAY = 1000\n",
        "random.seed(1)"
      ],
      "execution_count": 405,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeDatmtvNCMl",
        "outputId": "4d95cf80-b5e1-4dab-a263-477539752968"
      },
      "source": [
        "%%time\n",
        "world = GridWorld()\n",
        "T = Trajectories(num_samples=500, world = world, expert_q=expert_q)\n",
        "T.sample_trajectories()\n",
        "\n",
        "irl_solver = MaxCausalEntropy(T.trajectories, world, LR, DECAY)\n",
        "irl_solver.maxirl()"
      ],
      "execution_count": 462,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:76: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:100: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 20.6 s, sys: 5.38 ms, total: 20.6 s\n",
            "Wall time: 20.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBioMfCSAwVG",
        "outputId": "5de6e8a4-1fef-4e46-d177-e1762ef31669",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "plot_greedy_policy(world, irl_solver)"
      ],
      "execution_count": 465,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N70B_Ds48pv5",
        "outputId": "52f4b24d-a904-4e64-9460-ba85ef64348c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for s in range(irl_solver.n_states):\n",
        "  print(f\"State {irl_solver.world.point_to_state_coord(s)}: {irl_solver.R(s)}\")"
      ],
      "execution_count": 463,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State (0, 0): 0.0\n",
            "State (0, 1): 6.687164732011264e-55\n",
            "State (0, 2): 5.402242170182198e-53\n",
            "State (0, 3): 1.6888957321571927e-51\n",
            "State (0, 4): 3.9061150576268173e-50\n",
            "State (1, 0): 1.0811740662017832e-41\n",
            "State (1, 1): 1.0731124641710195e-41\n",
            "State (1, 2): 9.954879016638112e-42\n",
            "State (1, 3): 8.268015394109043e-42\n",
            "State (1, 4): 6.681933897822928e-42\n",
            "State (2, 0): 1.68998004488742e-39\n",
            "State (2, 1): 1.4308701654432994e-39\n",
            "State (2, 2): 9.161071034426474e-40\n",
            "State (2, 3): 3.065250869325197e-40\n",
            "State (2, 4): 8.273417947935976e-41\n",
            "State (3, 0): 1.7029776408844157e-37\n",
            "State (3, 1): 1.2200365482877706e-37\n",
            "State (3, 2): 3.444492331007845e-38\n",
            "State (3, 3): 2.1302946265109376e-39\n",
            "State (3, 4): 2.9809807389230563e-40\n",
            "State (4, 0): 1.143988322833901e-35\n",
            "State (4, 1): 7.061785043480832e-36\n",
            "State (4, 2): 1.3023922933037665e-36\n",
            "State (4, 3): 7.044124292957031e-38\n",
            "State (4, 4): 1.5384683341283727e-39\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}