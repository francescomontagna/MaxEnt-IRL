{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP5aMrZBamALqBHHwng0m+A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescomontagna/MaxEnt-IRL/blob/main/Basic-IRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SImPGvTum7X"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkDx9n_kE_Dm"
      },
      "source": [
        "# Classic RL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYgvXUEJFJXi"
      },
      "source": [
        "class Grid(object):\n",
        "\n",
        "  def __init__(self, noisy=False):\n",
        "    # -1: wall\n",
        "    # 0: empty, episode continues\n",
        "    # other: number indicates reward, episode will terminate\n",
        "    self._layout = np.array([\n",
        "      [-1, -1, -1, -1, -1, -1, -1],\n",
        "      [-1,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0, 10, -1],\n",
        "      [-1,  0,  0,  0,  0,  0, -1],\n",
        "      [-1, -1, -1, -1, -1, -1, -1],\n",
        "    ])\n",
        "    self._start_state = (1 , 1)\n",
        "    self._state = self._start_state\n",
        "    self._number_of_states = np.prod(np.shape(self._layout))\n",
        "    self._noisy = noisy\n",
        "\n",
        "  @property\n",
        "  def number_of_states(self):\n",
        "      return self._number_of_states\n",
        "    \n",
        "  def plot_grid(self):\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(self._layout > -1, interpolation=\"nearest\", cmap='pink')\n",
        "    ax = plt.gca() # 'get current axis'\n",
        "    ax.grid(0)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title(\"The grid\")\n",
        "    plt.text(2, 2, r\"$\\mathbf{S}$\", ha='center', va='center')\n",
        "    plt.text(8, 3, r\"$\\mathbf{G}$\", ha='center', va='center')\n",
        "    h, w = self._layout.shape\n",
        "    for y in range(h-1):\n",
        "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
        "    for x in range(w-1):\n",
        "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n",
        "\n",
        "  \n",
        "  def get_obs(self):\n",
        "    y, x = self._state\n",
        "    return y*self._layout.shape[1] + x\n",
        "\n",
        "  def obs_to_state(obs):\n",
        "    x = obs % self._layout.shape[1]\n",
        "    y = obs // self._layout.shape[1]\n",
        "    s = np.copy(grid._layout)\n",
        "    s[y, x] = 4\n",
        "    return s\n",
        "\n",
        "  # Environment: given an action as input, return [state, reward]\n",
        "  def step(self, action):\n",
        "    y, x = self._state\n",
        "    \n",
        "    # Next state\n",
        "    if action == 0:  # up\n",
        "      new_state = (y - 1, x)\n",
        "    elif action == 1:  # right\n",
        "      new_state = (y, x + 1)\n",
        "    elif action == 2:  # down\n",
        "      new_state = (y + 1, x)\n",
        "    elif action == 3:  # left\n",
        "      new_state = (y, x - 1)\n",
        "    else:\n",
        "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
        "\n",
        "    # Next reward\n",
        "    new_y, new_x = new_state\n",
        "    if self._layout[new_y, new_x] == -1:  # wall\n",
        "      reward = -5.\n",
        "      discount = 0.9\n",
        "      new_state = (y, x)\n",
        "    elif self._layout[new_y, new_x] == 0:  # empty cell\n",
        "      reward = 0.\n",
        "      discount = 0.9\n",
        "    else:  # a goal\n",
        "      reward = self._layout[new_y, new_x]\n",
        "      discount = 0.\n",
        "      new_state = self._start_state\n",
        "    \n",
        "    # Add noise to the reward (?)\n",
        "    if self._noisy:\n",
        "      width = self._layout.shape[1]\n",
        "      reward += 2*np.random.normal(0, width - new_x + new_y)\n",
        "    \n",
        "    self._state = new_state\n",
        "\n",
        "    return reward, discount, self.get_obs()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA3lPnivGFjt"
      },
      "source": [
        "def run_experiment(env, agent, number_of_steps):\n",
        "    mean_reward = 0.\n",
        "    try:\n",
        "      action = agent.initial_action()\n",
        "    except AttributeError:\n",
        "      action = 0\n",
        "    for i in range(number_of_steps):\n",
        "      reward, discount, next_state = grid.step(action)\n",
        "      action = agent.step(reward, discount, next_state)\n",
        "      mean_reward += (reward - mean_reward)/(i + 1.)\n",
        "\n",
        "    return mean_reward\n",
        "\n",
        "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
        "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
        "\n",
        "def plot_values(values, colormap='pink', vmin=0, vmax=10):\n",
        "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  plt.colorbar(ticks=[vmin, vmax])\n",
        "\n",
        "def plot_action_values(action_values, vmin=0, vmax=10):\n",
        "  q = action_values\n",
        "  fig = plt.figure(figsize=(8, 8))\n",
        "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "  for a in [0, 1, 2, 3]:\n",
        "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
        "    plot_values(q[..., a], vmin=vmin, vmax=vmax)\n",
        "    action_name = map_from_action_to_name(a)\n",
        "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
        "    \n",
        "  plt.subplot(3, 3, 5)\n",
        "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
        "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
        "  plt.title(\"$v(s)$\")\n",
        "\n",
        "\n",
        "def plot_rewards(xs, rewards, color):\n",
        "  mean = np.mean(rewards, axis=0)\n",
        "  p90 = np.percentile(rewards, 90, axis=0)\n",
        "  p10 = np.percentile(rewards, 10, axis=0)\n",
        "  plt.plot(xs, mean, color=color, alpha=0.6)\n",
        "  plt.fill_between(xs, p90, p10, color=color, alpha=0.3)\n",
        "  \n",
        "\n",
        "def parameter_study(parameter_values, parameter_name,\n",
        "  agent_constructor, env_constructor, color, repetitions=10, number_of_steps=int(1e4)):\n",
        "  mean_rewards = np.zeros((repetitions, len(parameter_values)))\n",
        "  greedy_rewards = np.zeros((repetitions, len(parameter_values)))\n",
        "  for rep in range(repetitions):\n",
        "    for i, p in enumerate(parameter_values):\n",
        "      env = env_constructor()\n",
        "      agent = agent_constructor()\n",
        "      if 'eps' in parameter_name:\n",
        "        agent.set_epsilon(p)\n",
        "      elif 'alpha' in parameter_name:\n",
        "        agent._step_size = p\n",
        "      else:\n",
        "        raise NameError(\"Unknown parameter_name: {}\".format(parameter_name))\n",
        "      mean_rewards[rep, i] = run_experiment(grid, agent, number_of_steps)\n",
        "      agent.set_epsilon(0.)\n",
        "      agent._step_size = 0.\n",
        "      greedy_rewards[rep, i] = run_experiment(grid, agent, number_of_steps//10)\n",
        "      del env\n",
        "      del agent\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plot_rewards(parameter_values, mean_rewards, color)\n",
        "  plt.yticks=([0, 1], [0, 1])\n",
        "  # plt.ylim((0, 1.5))\n",
        "  plt.ylabel(\"Average reward over first {} steps\".format(number_of_steps), size=12)\n",
        "  plt.xlabel(parameter_name, size=12)\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plot_rewards(parameter_values, greedy_rewards, color)\n",
        "  plt.yticks=([0, 1], [0, 1])\n",
        "  # plt.ylim((0, 1.5))\n",
        "  plt.ylabel(\"Final rewards, with greedy policy\".format(number_of_steps), size=12)\n",
        "  plt.xlabel(parameter_name, size=12)\n",
        "  \n",
        "def epsilon_greedy(q_values, epsilon):\n",
        "  if epsilon < np.random.random():\n",
        "    return np.argmax(q_values)\n",
        "  else:\n",
        "    return np.random.randint(np.array(q_values).shape[-1])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBWgU589Du8u"
      },
      "source": [
        "class classicRLSolver:\n",
        "  def __init__(self, number_of_states, number_of_actions, initial_state, target_policy, behaviour_policy, double, step_size=0.1):\n",
        "    self._q = np.zeros((number_of_states, number_of_actions))\n",
        "    if double:\n",
        "      self._q2 = np.zeros((number_of_states, number_of_actions))\n",
        "    self._s = initial_state\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._step_size = step_size\n",
        "    self._behaviour_policy = behaviour_policy\n",
        "    self._target_policy = target_policy\n",
        "    self._double = double\n",
        "    self._last_action = 0\n",
        "    \n",
        "  @property\n",
        "  def q_values(self):\n",
        "    if self._double:\n",
        "      return (self._q + self._q2)/2\n",
        "    else:\n",
        "      return self._q\n",
        "\n",
        "  def step(self, r, g, s):\n",
        "    \"\"\"\n",
        "    Params: \n",
        "    r (int): reward value sampled from the environment. Used for the update\n",
        "    g (float): discount factor\n",
        "    s (int): next state (t+1) sampled. Bootstrap from s\n",
        "    Returns:\n",
        "    next_action (int): refer to Grid() class for action encoding\n",
        "    \"\"\"\n",
        "\n",
        "    if self._double:\n",
        "      values = [self._q, self._q2]\n",
        "      random.shuffle(values)\n",
        "    else:\n",
        "      values = [self._q, self._q]\n",
        "    \n",
        "    a_prime = self._behaviour_policy(values[0][s])\n",
        "\n",
        "    # Next target action, used for the update\n",
        "    target_a_prime = self._target_policy(values[0][s], a_prime)\n",
        "\n",
        "    values[0][self._s, self._last_action] += self._step_size*\\\n",
        "      (r + g*np.dot(values[1][s], target_a_prime) - values[0][self._s, self._last_action])\n",
        "\n",
        "    # Next action: behaviour - in double-q, sample using updated value\n",
        "    self._s = s\n",
        "    self._last_action = a_prime\n",
        "\n",
        "    return a_prime"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "dv2xQMM-GOST",
        "outputId": "de7a8c34-6a95-43e0-e245-f55dd6ed2d9d"
      },
      "source": [
        "grid = Grid()\n",
        "\n",
        "def target_policy(q, a):\n",
        "  \"\"\"\n",
        "  :param q: action-value vector for a given state \n",
        "  \"\"\"\n",
        "  # One hot vector with one for the most valued action\n",
        "  return np.eye(len(q))[np.argmax(q)]\n",
        "\n",
        "def behaviour_policy(q):\n",
        "  return epsilon_greedy(q, 0.1)\n",
        "\n",
        "agent = classicRLSolver(grid._layout.size, 4, grid.get_obs(),\n",
        "                 target_policy, behaviour_policy, double=False)\n",
        "run_experiment(grid, agent, int(1e5))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tWvaVO8Ganu"
      },
      "source": [
        "expert_q = q[1:-1, 1:-1, :]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4DxLRFcFCYQ"
      },
      "source": [
        "# IRL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYcfdDjhuvZG"
      },
      "source": [
        "class GridWorld:\n",
        "  def __init__(self, p_initial, grid_size = 5):\n",
        "    self.actions = [0, 1, 2, 3] # [up, right, down, left]\n",
        "    self.grid_size = grid_size\n",
        "    self.terminal_state = (3, 4)\n",
        "    self.p_initial = p_initial # must be computed form trajectories\n",
        "\n",
        "    self.reset()\n",
        "\n",
        "  def state_coord_to_point(self, state_coord):\n",
        "    return state_coord[0]*self.grid_size + state_coord[1]\n",
        "\n",
        "  # TODO: test\n",
        "  def point_to_state_coord(self, point):\n",
        "    row = point // self.grid_size\n",
        "    col = point % grid_size\n",
        "    return row, col\n",
        "\n",
        "  #TODO: test\n",
        "  def simulate_step(self, state, action):\n",
        "    s_coord = self.point_to_state_coord(state)\n",
        "    next_s_coord = None\n",
        "\n",
        "    if action == 0:\n",
        "      if s_coord[0] == 0:\n",
        "        next_s_coord = s_coord\n",
        "      else:\n",
        "        next_s_coord = (s_coord[0]-1, s_coord[1])\n",
        "\n",
        "    elif action == 1:\n",
        "      if s_coord[1] == (self.grid_size-1):\n",
        "        next_s_coord = s_coord\n",
        "      else:\n",
        "        next_s_coord = (s_coord[0], s_coord[1]+1)\n",
        "      \n",
        "    elif action == 2:\n",
        "      if s_coord[0] == (self.grid_size-1):\n",
        "        next_s_coord = s_coord\n",
        "      else:\n",
        "        next_s_coord = (s_coord[0]+1, s_coord[1])\n",
        "\n",
        "    elif action == 3:\n",
        "      if s_coord[1] == 0:\n",
        "        next_s_coord = s_coord\n",
        "      else:\n",
        "        next_s_coord = (s_coord[0], s_coord[1]-1)\n",
        "\n",
        "    return self.state_coord_to_point(next_s_coord)\n",
        "\n",
        "  def step(self, state, action):\n",
        "    next_state = self.simulate_step(state, action)\n",
        "    self.state_coord = self.point_to_state_coord(next_state) \n",
        "\n",
        "    return self.state\n",
        "\n",
        "  # Initialize the gridworld\n",
        "  def reset(self):\n",
        "    initial_state = random.choice(range(self.grid_size**2), p=self.p_initial)\n",
        "    self.state_coord = self.state_coord_to_point(initial_state)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImrBgjMT8YNd"
      },
      "source": [
        "class Trajectories:\n",
        "  def __init__(self, num_samples, world, expert_q):\n",
        "    self.world = world\n",
        "    self.num_samples = num_samples\n",
        "    self.trajectories = []\n",
        "    self.expert_q = expert_q\n",
        "\n",
        "    self.s = self.world.state\n",
        "\n",
        "  def sample_trajectories(self):\n",
        "    terminal_state = self.world.terminal # coord\n",
        "    for _ in range(num_samples):\n",
        "      self.world.reset()\n",
        "      self.s = \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_DeRVGZxKJC"
      },
      "source": [
        "class MaxCausalEntropy:\n",
        "  # We'll consider the case of a deterministic policy\n",
        "  def __init__(self, trajectories, lr, decay):\n",
        "    \"\"\"\n",
        "    Attributes:\n",
        "      trajectories: list of the sampeld trajectories\n",
        "      lr: learning rate for SGA\n",
        "      decay: weight decay for SGA\n",
        "      world: The environment modeled as GridWorld\n",
        "      features: feature to represrent each state. Simply encoded as coordinates in the grid\n",
        "      theta: learnable parameters, one vector for each possible state. Same dimensionality of features\n",
        "      V: (n_states x n_states) matrix with value associated to each state\n",
        "      D: (n_states x n_states) matrix with visitation frequencies\n",
        "      gamma: discount (Forse dev'essere ritornato da simulate_step, decido poi)\n",
        "    \"\"\"\n",
        "    self.trajectories = trajectories\n",
        "    self.lr = lr\n",
        "    self.decay = decay\n",
        "\n",
        "    self.world = GridWorld()\n",
        "    self.n_states = self.world.grid_size**2\n",
        "    self.features = np.zeros((self.n_states, 2))\n",
        "    for s in range(self.n_states):\n",
        "      i, j = self.world.point_to_state_coord(s)\n",
        "      self.features[s, 0] = i\n",
        "      self.features[s, 1] = j\n",
        "\n",
        "    self.theta = np.zeros((self.n_states, 2)) # np.matmul(self.theta[i, j], self.features[i, j]) = R(s=[i,j])\n",
        "    self.V = np.zeros((self.n_states))\n",
        "    self.D = np.zeros((self.n_states))\n",
        "\n",
        "    self.gamma = 0.9\n",
        "    \n",
        "\n",
        "  def R(self, state):\n",
        "    \"\"\"\n",
        "    Return reward associated to given state (point)\n",
        "    \"\"\"\n",
        "    return np.matmul(self.theta[s], self.features[s])\n",
        "\n",
        "  # def Q(self, state, action):\n",
        "  #   \"\"\"\n",
        "  #   Return action value associated to given state (point)\n",
        "  #   Maximize over action to get Q(S, A)\n",
        "  #   \"\"\"\n",
        "  #   next_s = self.world.simulate_step(state, action)\n",
        "  #   return self.R(state) + self.gamma * self.V[next_s]\n",
        "\n",
        "  def policy(self, state, action):\n",
        "    return np.exp(self.Q(state, action) - self.V[state])\n",
        "    \n",
        "  def feature_exp_from_trajectories(self):\n",
        "    # Compute feature_exp, as a representation of expert behaviour\n",
        "    _, features_dim = self.features.shape\n",
        "    f_exp = np.zeros((features_dim))\n",
        "\n",
        "    for trajectory in self.trajectories:\n",
        "      for step in trajectory:\n",
        "        state, action = step\n",
        "        f_exp += self.features[state] # TODO: check if it's point or coord\n",
        "\n",
        "    return f_exp / len(self.trajectories)\n",
        "\n",
        "\n",
        "  def pseudo_gpi(self):\n",
        "    \"\"\"\n",
        "    Algorithm to update the value function\n",
        "    \"\"\"\n",
        "    # NOTE: state = point in range(self.num_states)\n",
        "    eps = 1e-3\n",
        "    \n",
        "    # For each state, define set V to -inf\n",
        "    self.V = np.ones((self.n_states))*np.min\n",
        "\n",
        "    # Set V_prime to -inf except for termnal state which is set to 0\n",
        "    V_prime = np.ones((self.n_states))*np.min\n",
        "    v_prime[self.world.state_coord_to_point(self.terminal_state)] = 0\n",
        "\n",
        "    delta = np.ones((self.num_states))*np.max # parameter to monitor convergence\n",
        "    while np.max(delta) > eps:\n",
        "      # Update V for each state\n",
        "      for s in range(self.num_states):\n",
        "        for a in self.world.actions:\n",
        "          next_s = self.world.simulate_step(s, a)\n",
        "          V_prime[s] = np.log(np.exp(V_prime[s]) + np.exp(self.R(s) + self.V[next_s]))\n",
        "\n",
        "        delta[s] = np.abs(self.V[s] - V_prime[s])\n",
        "        self.V[s] = V_prime[s]\n",
        "\n",
        "    # return self.V \n",
        "\n",
        "  def expected_svf(self):\n",
        "    eps = 1e-5\n",
        "\n",
        "    self.D = np.zeros((n_states))\n",
        "    D_prime = world.p_initial # TODO Handle copy (shallow, deep, .. boh)\n",
        "\n",
        "    delta = np.ones((self.num_states))*np.max\n",
        "    while np.max(delta) > eps:\n",
        "      for s in range(self.num_states):\n",
        "        for a in self.world.actions:\n",
        "          next_s = self.world.simulate_step(s, a)\n",
        "          D_prime[s] += self.D[s]*self.policy(next_s, a)\n",
        "\n",
        "        delta[s] = np.abs(self.D[s] - D_prime[s])\n",
        "        self.D[s] = D_prime[s]\n",
        "           \n",
        "    # return self.D[s]\n",
        "\n",
        "  def sga(self):\n",
        "    eps = 1e-4\n",
        "    delta = np.zeros(self.features.shape)\n",
        "    decay = self.decay\n",
        "    f_exp_from_t  = self.feature_exp_from_trajectories()\n",
        "    while np.max(delta) > eps:\n",
        "      for state in range(self.num_states):\n",
        "        gradient = self.features[state]*self.D[state] # Not sure this is right..., non c'è traccia delle azioni\n",
        "        update = np.exp(-(self.lr/decay)*(f_exp_from_t - gradient))\n",
        "        delta[state] = np.abs(self.theta[state] - self.theta[state]*update)\n",
        "        self.theta[state] *= update\n",
        "\n",
        "    # return self.theta"
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}