{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMVibc6JnKZPz38vuauQfU9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescomontagna/MaxEnt-IRL/blob/main/Basic-IRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SImPGvTum7X"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from itertools import product"
      ],
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkDx9n_kE_Dm"
      },
      "source": [
        "# Classic RL\n",
        "Use TD Learning to learn a policy that will be used as the expert policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA3lPnivGFjt"
      },
      "source": [
        "def run_experiment(env, agent, number_of_steps):\n",
        "    mean_reward = 0.\n",
        "    try:\n",
        "      action = agent.initial_action()\n",
        "    except AttributeError:\n",
        "      action = 0\n",
        "    for i in range(number_of_steps):\n",
        "      reward, discount, next_state = grid.step(action)\n",
        "      action = agent.step(reward, discount, next_state)\n",
        "      mean_reward += (reward - mean_reward)/(i + 1.)\n",
        "\n",
        "    return mean_reward\n",
        "\n",
        "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
        "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
        "\n",
        "def plot_values(values, colormap='pink', vmin=0, vmax=10):\n",
        "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  plt.colorbar(ticks=[vmin, vmax])\n",
        "\n",
        "def plot_action_values(action_values, vmin=0, vmax=10):\n",
        "  q = action_values\n",
        "  fig = plt.figure(figsize=(8, 8))\n",
        "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "  for a in [0, 1, 2, 3]:\n",
        "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
        "    plot_values(q[..., a], vmin=vmin, vmax=vmax)\n",
        "    action_name = map_from_action_to_name(a)\n",
        "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
        "    \n",
        "  plt.subplot(3, 3, 5)\n",
        "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
        "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
        "  plt.title(\"$v(s)$\")\n",
        "\n",
        "\n",
        "def plot_rewards(xs, rewards, color):\n",
        "  mean = np.mean(rewards, axis=0)\n",
        "  p90 = np.percentile(rewards, 90, axis=0)\n",
        "  p10 = np.percentile(rewards, 10, axis=0)\n",
        "  plt.plot(xs, mean, color=color, alpha=0.6)\n",
        "  plt.fill_between(xs, p90, p10, color=color, alpha=0.3)\n",
        "  \n",
        "\n",
        "def parameter_study(parameter_values, parameter_name,\n",
        "  agent_constructor, env_constructor, color, repetitions=10, number_of_steps=int(1e4)):\n",
        "  mean_rewards = np.zeros((repetitions, len(parameter_values)))\n",
        "  greedy_rewards = np.zeros((repetitions, len(parameter_values)))\n",
        "  for rep in range(repetitions):\n",
        "    for i, p in enumerate(parameter_values):\n",
        "      env = env_constructor()\n",
        "      agent = agent_constructor()\n",
        "      if 'eps' in parameter_name:\n",
        "        agent.set_epsilon(p)\n",
        "      elif 'alpha' in parameter_name:\n",
        "        agent._step_size = p\n",
        "      else:\n",
        "        raise NameError(\"Unknown parameter_name: {}\".format(parameter_name))\n",
        "      mean_rewards[rep, i] = run_experiment(grid, agent, number_of_steps)\n",
        "      agent.set_epsilon(0.)\n",
        "      agent._step_size = 0.\n",
        "      greedy_rewards[rep, i] = run_experiment(grid, agent, number_of_steps//10)\n",
        "      del env\n",
        "      del agent\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plot_rewards(parameter_values, mean_rewards, color)\n",
        "  plt.yticks=([0, 1], [0, 1])\n",
        "  # plt.ylim((0, 1.5))\n",
        "  plt.ylabel(\"Average reward over first {} steps\".format(number_of_steps), size=12)\n",
        "  plt.xlabel(parameter_name, size=12)\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plot_rewards(parameter_values, greedy_rewards, color)\n",
        "  plt.yticks=([0, 1], [0, 1])\n",
        "  # plt.ylim((0, 1.5))\n",
        "  plt.ylabel(\"Final rewards, with greedy policy\".format(number_of_steps), size=12)\n",
        "  plt.xlabel(parameter_name, size=12)\n",
        "  \n",
        "def epsilon_greedy(q_values, epsilon):\n",
        "  if epsilon < np.random.random():\n",
        "    return np.argmax(q_values)\n",
        "  else:\n",
        "    return np.random.randint(np.array(q_values).shape[-1])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYgvXUEJFJXi"
      },
      "source": [
        "class Grid(object):\n",
        "\n",
        "  def __init__(self, noisy=False):\n",
        "    # -1: wall\n",
        "    # 0: empty, episode continues\n",
        "    # other: number indicates reward, episode will terminate\n",
        "    self._layout = np.array([\n",
        "      [-1, -1, -1, -1, -1, -1, -1],\n",
        "      [-1,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0, 10, -1],\n",
        "      [-1,  0,  0,  0,  0,  0, -1],\n",
        "      [-1, -1, -1, -1, -1, -1, -1],\n",
        "    ])\n",
        "    self._start_state = random.choices(range(1, self._layout.shape[0]-1), k=2)\n",
        "    while self._start_state == [4, 5]:\n",
        "      self._start_state = random.choices(range(1, self._layout.shape[0]-1), k=2)\n",
        "    self._state = self._start_state\n",
        "    self._number_of_states = np.prod(np.shape(self._layout))\n",
        "    self._noisy = noisy\n",
        "\n",
        "  @property\n",
        "  def number_of_states(self):\n",
        "      return self._number_of_states\n",
        "    \n",
        "  def plot_grid(self):\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(self._layout > -1, interpolation=\"nearest\", cmap='pink')\n",
        "    ax = plt.gca() # 'get current axis'\n",
        "    ax.grid(0)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title(\"The grid\")\n",
        "    plt.text(2, 2, r\"$\\mathbf{S}$\", ha='center', va='center')\n",
        "    plt.text(8, 3, r\"$\\mathbf{G}$\", ha='center', va='center')\n",
        "    h, w = self._layout.shape\n",
        "    for y in range(h-1):\n",
        "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
        "    for x in range(w-1):\n",
        "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n",
        "\n",
        "  \n",
        "  def get_obs(self):\n",
        "    y, x = self._state\n",
        "    return y*self._layout.shape[1] + x\n",
        "\n",
        "  def obs_to_state(obs):\n",
        "    x = obs % self._layout.shape[1]\n",
        "    y = obs // self._layout.shape[1]\n",
        "    s = np.copy(grid._layout)\n",
        "    s[y, x] = 4\n",
        "    return s\n",
        "\n",
        "  # Environment: given an action as input, return [state, reward]\n",
        "  def step(self, action):\n",
        "    y, x = self._state\n",
        "    \n",
        "    # Next state\n",
        "    if action == 0:  # up\n",
        "      new_state = (y - 1, x)\n",
        "    elif action == 1:  # right\n",
        "      new_state = (y, x + 1)\n",
        "    elif action == 2:  # down\n",
        "      new_state = (y + 1, x)\n",
        "    elif action == 3:  # left\n",
        "      new_state = (y, x - 1)\n",
        "    else:\n",
        "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
        "\n",
        "    # Next reward\n",
        "    new_y, new_x = new_state\n",
        "    if self._layout[new_y, new_x] == -1:  # wall\n",
        "      reward = -5.\n",
        "      discount = 0.9\n",
        "      new_state = (y, x)\n",
        "    elif self._layout[new_y, new_x] == 0:  # empty cell\n",
        "      reward = -1\n",
        "      discount = 0.9\n",
        "    else:  # a goal\n",
        "      reward = self._layout[new_y, new_x]\n",
        "      discount = 0.\n",
        "      new_state = self._start_state\n",
        "    \n",
        "    # Add noise to the reward (?)\n",
        "    if self._noisy:\n",
        "      width = self._layout.shape[1]\n",
        "      reward += 2*np.random.normal(0, width - new_x + new_y)\n",
        "    \n",
        "    self._state = new_state\n",
        "\n",
        "    return reward, discount, self.get_obs()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBWgU589Du8u"
      },
      "source": [
        "class classicRLSolver:\n",
        "  def __init__(self, number_of_states, number_of_actions, initial_state, target_policy, behaviour_policy, double, step_size=0.1):\n",
        "    self._q = np.zeros((number_of_states, number_of_actions))\n",
        "    if double:\n",
        "      self._q2 = np.zeros((number_of_states, number_of_actions))\n",
        "    self._s = initial_state\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._step_size = step_size\n",
        "    self._behaviour_policy = behaviour_policy\n",
        "    self._target_policy = target_policy\n",
        "    self._double = double\n",
        "    self._last_action = 0\n",
        "    \n",
        "  @property\n",
        "  def q_values(self):\n",
        "    if self._double:\n",
        "      return (self._q + self._q2)/2\n",
        "    else:\n",
        "      return self._q\n",
        "\n",
        "  def step(self, r, g, s):\n",
        "    \"\"\"\n",
        "    Params: \n",
        "    r (int): reward value sampled from the environment. Used for the update\n",
        "    g (float): discount factor\n",
        "    s (int): next state (t+1) sampled. Bootstrap from s\n",
        "    Returns:\n",
        "    next_action (int): refer to Grid() class for action encoding\n",
        "    \"\"\"\n",
        "\n",
        "    if self._double:\n",
        "      values = [self._q, self._q2]\n",
        "      random.shuffle(values)\n",
        "    else:\n",
        "      values = [self._q, self._q]\n",
        "    \n",
        "    a_prime = self._behaviour_policy(values[0][s])\n",
        "\n",
        "    # Next target action, used for the update\n",
        "    target_a_prime = self._target_policy(values[0][s], a_prime)\n",
        "\n",
        "    values[0][self._s, self._last_action] += self._step_size*\\\n",
        "      (r + g*np.dot(values[1][s], target_a_prime) - values[0][self._s, self._last_action])\n",
        "\n",
        "    # Next action: behaviour - in double-q, sample using updated value\n",
        "    self._s = s\n",
        "    self._last_action = a_prime\n",
        "\n",
        "    return a_prime"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "dv2xQMM-GOST",
        "outputId": "6769bd3c-5eb8-4bbb-99ad-58dbef36455c"
      },
      "source": [
        "grid = Grid()\n",
        "\n",
        "def target_policy(q, a):\n",
        "  \"\"\"\n",
        "  :param q: action-value vector for a given state \n",
        "  \"\"\"\n",
        "  # One hot vector with one for the most valued action\n",
        "  return np.eye(len(q))[np.argmax(q)]\n",
        "\n",
        "def behaviour_policy(q):\n",
        "  return epsilon_greedy(q, 0.1)\n",
        "\n",
        "agent = classicRLSolver(grid._layout.size, 4, grid.get_obs(),\n",
        "                 target_policy, behaviour_policy, double=False)\n",
        "run_experiment(grid, agent, int(1e5))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tWvaVO8Ganu"
      },
      "source": [
        "expert_q = q[1:-1, 1:-1, :]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4DxLRFcFCYQ"
      },
      "source": [
        "# IRL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E96ZGzb1vraH"
      },
      "source": [
        "def plot_trajectory(world, t):\n",
        "  plt.figure(figsize=(3, 3))\n",
        "  plt.imshow(world.layout > -1, interpolation=\"nearest\")     \n",
        "  ax = plt.gca()\n",
        "  ax.grid(0)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.title(\"The grid\")\n",
        "\n",
        "  start = t[0][0]\n",
        "  end = t[-1][0]\n",
        "  plt.text(\n",
        "      start[1], start[0], \n",
        "      r\"$\\mathbf{S}$\", ha='center', va='center')\n",
        "  plt.text(\n",
        "      end[1], end[0], \n",
        "      r\"$\\mathbf{G}$\", ha='center', va='center')\n",
        "\n",
        "  action_names = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
        "\n",
        "  for state, action in t[:-1]:\n",
        "    action_name = action_names[action]\n",
        "    plt.text(state[1], state[0], action_name, ha='center', va='center')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYcfdDjhuvZG"
      },
      "source": [
        "class GridWorld:\n",
        "  def __init__(self, grid_size = 5):\n",
        "    \"\"\"\n",
        "    Attributes:\n",
        "      actions: actions available to the agent (one step in the grid)\n",
        "      grid_size: size of the side of teh squared grid\n",
        "      terminal_state: terminal state of the environment\n",
        "      p_initial: ... ?\n",
        "      state: state in terms of point ion range(0, geid_size**2)\n",
        "      state_coord: coordinates of the state in the grid\n",
        "    \"\"\"\n",
        "\n",
        "    self.actions = [0, 1, 2, 3] # [up, right, down, left]\n",
        "    self.grid_size = grid_size\n",
        "    self.layout = np.zeros((grid_size, grid_size))\n",
        "    self.terminal_state = self.state_coord_to_point((3, 4))\n",
        "    self.p_initial = np.zeros(self.layout.shape)*(1/self.grid_size**2) # must be computed form trajectories\n",
        "    self.gamma = 0.9\n",
        "\n",
        "    self.reset()\n",
        "\n",
        "  def state_coord_to_point(self, state_coord):\n",
        "    return state_coord[0]*self.grid_size + state_coord[1]\n",
        "\n",
        "  # TODO: test\n",
        "  def point_to_state_coord(self, point):\n",
        "\n",
        "    row = point // self.grid_size\n",
        "    col = point % self.grid_size\n",
        "    return row, col\n",
        "\n",
        "  #TODO: test\n",
        "  def simulate_step(self, state, action):\n",
        "    state_coord = self.point_to_state_coord(state)\n",
        "    next_s_coord = None\n",
        "\n",
        "    if action == 0:\n",
        "      if state_coord[0] == 0:\n",
        "        next_s_coord = state_coord\n",
        "      else:\n",
        "        next_s_coord = (state_coord[0]-1, state_coord[1])\n",
        "\n",
        "    elif action == 1:\n",
        "      if state_coord[1] == (self.grid_size-1):\n",
        "        next_s_coord = state_coord\n",
        "      else:\n",
        "        next_s_coord = (state_coord[0], state_coord[1]+1)\n",
        "      \n",
        "    elif action == 2:\n",
        "      if state_coord[0] == (self.grid_size-1):\n",
        "        next_s_coord = state_coord\n",
        "      else:\n",
        "        next_s_coord = (state_coord[0]+1, state_coord[1])\n",
        "\n",
        "    elif action == 3:\n",
        "      if state_coord[1] == 0:\n",
        "        next_s_coord = state_coord\n",
        "      else:\n",
        "        next_s_coord = (state_coord[0], state_coord[1]-1)\n",
        "\n",
        "    if self.state_coord_to_point(next_s_coord) == self.terminal_state:\n",
        "      # return self.state_coord_to_point(self.initial_state), 0\n",
        "      return self.state_coord_to_point(next_s_coord), 0\n",
        "\n",
        "    return self.state_coord_to_point(next_s_coord), self.gamma\n",
        "\n",
        "  def step(self, action):\n",
        "    next_state, gamma = self.simulate_step(self.state, action)\n",
        "    self.state = next_state\n",
        "\n",
        "    return next_state \n",
        "\n",
        "  # Initialize the gridworld\n",
        "  def reset(self):\n",
        "    initial_state = random.choice(range(self.grid_size**2))\n",
        "    while (self.point_to_state_coord(initial_state)) == self.terminal_state:\n",
        "      initial_state = random.choice(range(self.grid_size**2))\n",
        "    self.state = initial_state\n",
        "    self.initial_state = initial_state"
      ],
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImrBgjMT8YNd"
      },
      "source": [
        "class Trajectories:\n",
        "  def __init__(self, num_samples, world, expert_q):\n",
        "    self.world = world\n",
        "    self.num_samples = num_samples\n",
        "    self.trajectories = []\n",
        "    self.expert_q = expert_q\n",
        "\n",
        "    self.s = self.world.state # point\n",
        "\n",
        "  def sample_trajectories(self):\n",
        "    \n",
        "    terminal_state = self.world.terminal_state # point\n",
        "    for _ in range(self.num_samples):\n",
        "      self.world.reset()\n",
        "      state = self.world.state\n",
        "      trajectory = []\n",
        "      while state != terminal_state:\n",
        "        state_coord = self.world.point_to_state_coord(state)\n",
        "        row, col = state_coord\n",
        "        action = np.argmax(self.expert_q[row, col])\n",
        "        trajectory.append((state_coord, action))\n",
        "        state = self.world.step(action)\n",
        "\n",
        "      # Do not store the terminal state, since it is unique\n",
        "\n",
        "      trajectory.append((self.world.point_to_state_coord(state), None))\n",
        "      self.trajectories.append(trajectory)"
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_DeRVGZxKJC"
      },
      "source": [
        "class MaxCausalEntropy:\n",
        "  # We'll consider the case of a deterministic policy\n",
        "  def __init__(self, trajectories, world, lr, decay):\n",
        "    \"\"\"\n",
        "    Attributes:\n",
        "      trajectories: list of the sampeld trajectories\n",
        "      lr: learning rate for SGA\n",
        "      decay: weight decay for SGA\n",
        "      world: The environment modeled as GridWorld\n",
        "      features: feature to represrent each state. Simply encoded as coordinates in the grid\n",
        "      theta: learnable parameters, one vector for each possible state. Same dimensionality of features\n",
        "      V: (n_states x n_states) matrix with value associated to each state\n",
        "      D: (n_states x n_states) matrix with visitation frequencies\n",
        "      gamma: discount (Forse dev'essere ritornato da simulate_step, decido poi)\n",
        "    \"\"\"\n",
        "    self.trajectories = trajectories\n",
        "    self.lr = lr\n",
        "    self.decay = decay\n",
        "    self.world = world\n",
        "\n",
        "    self.n_states = self.world.grid_size**2\n",
        "    self.n_actions = len(self.world.actions)\n",
        "    self.features = np.zeros((self.n_states, 2))\n",
        "    for s in range(self.n_states):\n",
        "      i, j = self.world.point_to_state_coord(s)\n",
        "      self.features[s, 0] = i\n",
        "      self.features[s, 1] = j\n",
        "\n",
        "    self.theta = np.ones((self.n_states, 2))/self.n_states # np.matmul(self.theta[i, j], self.features[i, j]) = R(s=[i,j])\n",
        "    self.V = np.zeros((self.n_states))\n",
        "    self.D = np.zeros((self.n_states))\n",
        "\n",
        "\n",
        "  ######################## HELPER FUNCITONS ########################\n",
        "\n",
        "  def R(self, state):\n",
        "    \"\"\"\n",
        "    Return reward associated to given state (point)\n",
        "    \"\"\"\n",
        "    return np.matmul(self.theta[state], self.features[state])\n",
        "\n",
        "  # TODO Verify\n",
        "  def Q(self, state, action):\n",
        "    \"\"\"\n",
        "    Return action value associated to given state (point)\n",
        "    Maximize over action to get Q(S, A)\n",
        "    \"\"\"\n",
        "    next_s, gamma = self.world.simulate_step(state, action)\n",
        "    return self.R(state) + gamma * self.V[next_s]\n",
        "\n",
        "  def policy(self, state, action):\n",
        "    return np.exp(self.Q(state, action) - self.V[state])\n",
        "    \n",
        "  # f_b: the feature vectpr representing the expert policy\n",
        "  def feature_exp_from_trajectories(self):\n",
        "    _, features_dim = self.features.shape\n",
        "    f_exp = np.zeros((features_dim))\n",
        "\n",
        "    for trajectory in self.trajectories:\n",
        "      for step in trajectory:\n",
        "        state, action = step\n",
        "        f_exp += self.features[state] # TODO: check if it's point or coord\n",
        "\n",
        "    return f_exp / len(self.trajectories)\n",
        "\n",
        "  def initial_probabilities_from_trajectories(self, n_states, trajectories):\n",
        "    p = np.zeros(self.world.layout.shape)\n",
        "\n",
        "    for t in trajectories: # for each trajectory\n",
        "      p[t[0][0][0], t[0][0][1]] += 1     # increment starting state\n",
        "\n",
        "    return p / len(trajectories)    \n",
        "\n",
        "  def softmax(self, x, y):\n",
        "    return np.log(np.exp(x) + np.exp(y))   \n",
        "\n",
        "\n",
        "  ######################## CORE FUNCTIONS ########################\n",
        "\n",
        "  def pseudo_gpi(self):\n",
        "    \"\"\"\n",
        "    Algorithm to update the value function\n",
        "    \"\"\"\n",
        "    eps = 1e-3\n",
        "    self.V = np.ones((self.n_states))*np.finfo(np.float32).min\n",
        "\n",
        "    delta = 2*eps\n",
        "    while delta > eps:\n",
        "      delta = 0\n",
        "      V_prime = np.ones((self.n_states))*np.finfo(np.float32).min\n",
        "      V_prime[self.world.terminal_state] = 0\n",
        "\n",
        "      # For each state and action, update the value function\n",
        "      for s in range(self.n_states):\n",
        "        for a in range(self.n_actions):\n",
        "          V_prime[s] = self.softmax(V_prime[s], self.Q(s, a)) # Il secondo termine è corretto? Dovrebbe esser Q(S, A)...\n",
        "\n",
        "      for s in range(self.n_states):\n",
        "        delta = max(np.abs(self.V[s] - V_prime[s]), delta)\n",
        "        self.V[s] = V_prime[s]\n",
        "\n",
        "\n",
        "  def expected_svf(self):\n",
        "    eps = 1e-5\n",
        "    self.D = np.zeros((self.n_states))\n",
        "\n",
        "    # Backward Pass\n",
        "    # 1. initialize at terminal states\n",
        "    zs = np.zeros(self.n_states)                             # zs: state partition function\n",
        "    zs[self.world.terminal_state] = 1.0\n",
        "\n",
        "    # 2. perform backward pass\n",
        "    for _ in range(2 * self.n_states):                       # longest trajectory: n_states\n",
        "        # reset action values to zero\n",
        "        za = np.zeros((self.n_states, self.n_actions))            # za: action partition function\n",
        "\n",
        "        # for each state-action pair\n",
        "        for s_from, a in product(range(self.n_states), range(self.n_actions)):\n",
        "\n",
        "            # sum over s_to\n",
        "            s_to, _ = self.world.simulate_step(s_from, a)\n",
        "            za[s_from, a] += np.exp(self.R(s_from)) * zs[s_to]\n",
        "        \n",
        "        # sum over all actions\n",
        "        zs = za.sum(axis=1)\n",
        "\n",
        "    # 3. compute local action probabilities\n",
        "    p_action = za / zs[:, None]\n",
        "\n",
        "    nonterminal = set(range(self.n_states)) - set([self.world.terminal_state])  # nonterminal states\n",
        "\n",
        "    delta = 2*eps\n",
        "    while delta > eps:\n",
        "      delta = 0\n",
        "      D_prime = np.reshape(\n",
        "          self.initial_probabilities_from_trajectories(self.n_states, self.trajectories),\n",
        "          (self.n_states)\n",
        "      )\n",
        "\n",
        "      # For each state and action, update the value function\n",
        "      for s in nonterminal:\n",
        "        for a in range(len(self.world.actions)):\n",
        "          next_s, gamma = self.world.simulate_step(s, a)\n",
        "          D_prime[next_s] += self.D[s]*p_action[s,a]\n",
        "\n",
        "      for s in range(self.n_states):\n",
        "        delta = max(np.abs(self.D[s] - D_prime[s]), delta)\n",
        "        self.D[s] = D_prime[s]\n",
        "      print(delta)\n",
        "\n",
        "    # return self.D[s]\n",
        "\n",
        "  def compute_expected_svf(self):\n",
        "\n",
        "    nonterminal = set(range(self.n_states)) - set([self.world.terminal_state])  # nonterminal states\n",
        "    \n",
        "    # Backward Pass\n",
        "    # 1. initialize at terminal states\n",
        "    zs = np.zeros(self.n_states)                             # zs: state partition function\n",
        "    zs[self.world.terminal_state] = 1.0\n",
        "\n",
        "    # 2. perform backward pass\n",
        "    for _ in range(2 * self.n_states):                       # longest trajectory: n_states\n",
        "        # reset action values to zero\n",
        "        za = np.zeros((self.n_states, self.n_actions))            # za: action partition function\n",
        "\n",
        "        # for each state-action pair\n",
        "        for s_from, a in product(range(self.n_states), range(self.n_actions)):\n",
        "\n",
        "            # sum over s_to\n",
        "            s_to, _ = self.world.simulate_step(s_from, a)\n",
        "            za[s_from, a] += np.exp(self.R(s_from)) * zs[s_to]\n",
        "        \n",
        "        # sum over all actions\n",
        "        zs = za.sum(axis=1)\n",
        "\n",
        "    # 3. compute local action probabilities\n",
        "    p_action = za / zs[:, None]\n",
        "\n",
        "    # Forward Pass\n",
        "    # 4. initialize with starting probability\n",
        "    d = np.zeros((self.n_states, 2 * self.n_states))              # d: state-visitation frequencies\n",
        "    d[:, 0] = np.reshape(\n",
        "                  self.initial_probabilities_from_trajectories(self.n_states, self.trajectories),\n",
        "                  (self.n_states)\n",
        "              )\n",
        "\n",
        "    # 5. iterate for N steps\n",
        "    for t in range(1, 2 * self.n_states):                    # longest trajectory: n_states\n",
        "        delta = 0\n",
        "        # sum over nonterminal state-action pairs\n",
        "        for s_from, a in product(nonterminal, range(self.n_actions)):\n",
        "            s_to, _ = self.world.simulate_step(s_from, a)\n",
        "            delta = max((d[s_to, t] + d[s_from, t-1] * p_action[s_from, a] - d[s_from, t-1]), delta)\n",
        "            d[s_to, t] += d[s_from, t-1] * p_action[s_from, a] \n",
        "        print(delta)\n",
        "    # 6. sum-up frequencies\n",
        "    return d.sum(axis=1)\n",
        "\n",
        "  def sga(self):\n",
        "    eps = 1e-4\n",
        "    delta = np.ones(self.features.shape)*np.finfo(float).max\n",
        "    decay = self.decay\n",
        "    f_exp_from_t  = self.feature_exp_from_trajectories()\n",
        "    while np.max(delta) > eps:\n",
        "      for state in range(self.n_states):\n",
        "        gradient = self.features[state]*self.D[state] # Not sure this is right..., non c'è traccia delle azioni\n",
        "        update = np.exp(-(self.lr/decay)*(f_exp_from_t - gradient))\n",
        "        delta[state] = np.abs(self.theta[state] - self.theta[state]*update)\n",
        "        self.theta[state] *= update\n",
        "\n",
        "    # return self.theta\n",
        "\n",
        "\n",
        "  ######################## ALGORITHM ########################\n",
        "\n",
        "  def maxirl(self):\n",
        "    eps = 1e-5\n",
        "    delta = 2*eps\n",
        "    while delta > eps:\n",
        "      self.pseudo_gpi()\n",
        "      print(self.compute_expected_svf())\n",
        "      self.expected_svf()\n",
        "      # self.sga()\n",
        "      delta/=10\n",
        "      \n",
        "    return self.V"
      ],
      "execution_count": 358,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdjeHQPJNSn5"
      },
      "source": [
        "# Experiements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8foWEeNNVZk"
      },
      "source": [
        "LR = 0.001\n",
        "DECAY = 1000\n",
        "random.seed(1)"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeDatmtvNCMl",
        "outputId": "3f7d20c2-2816-4e68-9d97-349f8dd96f41"
      },
      "source": [
        "world = GridWorld()\n",
        "T = Trajectories(num_samples=200, world = world, expert_q=expert_q)\n",
        "T.sample_trajectories()\n",
        "\n",
        "irl_solver = MaxCausalEntropy(T.trajectories, world, LR, DECAY)\n",
        "_ = irl_solver.maxirl()\n",
        "print(irl_solver.D)"
      ],
      "execution_count": 359,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.027193089322179002\n",
            "0.01956260020727623\n",
            "0.014670963689218323\n",
            "0.017991328053928314\n",
            "0.017378482072991565\n",
            "0.01909747980117793\n",
            "0.017931197123799106\n",
            "0.017645693938522132\n",
            "0.015884047958915362\n",
            "0.014724748871925174\n",
            "0.01292508331329649\n",
            "0.011608322412555251\n",
            "0.010060804977483274\n",
            "0.008888337648854702\n",
            "0.00766601053351415\n",
            "0.00672112055046812\n",
            "0.0057973967287543865\n",
            "0.005071807246783522\n",
            "0.004389177728884246\n",
            "0.0038450186217299023\n",
            "0.0033452454337581095\n",
            "0.002941130343221153\n",
            "0.002575557603138476\n",
            "0.0022757812616348017\n",
            "0.002007054143472988\n",
            "0.0017836321599439993\n",
            "0.0015843042306296837\n",
            "0.00141633826667752\n",
            "0.0012667227672323034\n",
            "0.0011390092913139034\n",
            "0.0010251638270780648\n",
            "0.0009267942605992001\n",
            "0.0008388980829045697\n",
            "0.0007620925113924377\n",
            "0.0006932297305209771\n",
            "0.0006324425769381366\n",
            "0.0005777246312600459\n",
            "0.0005289886153230436\n",
            "0.00048493573287764456\n",
            "0.00044539290397280044\n",
            "0.00040950402926668433\n",
            "0.0003770756629851643\n",
            "0.00034753172262326146\n",
            "0.0003206880079606334\n",
            "0.00029614789447936045\n",
            "0.0002737479224346421\n",
            "0.0002532082665579827\n",
            "0.00023438893960785537\n",
            "0.00021708749729843543\n",
            "[0.23366117 0.26867963 0.33991343 0.45637152 0.57398519 0.32077671\n",
            " 0.37652002 0.47655374 0.63575151 0.7197301  0.38552505 0.50877633\n",
            " 0.70688687 0.86507668 0.79709097 0.50936861 0.68538787 1.03045929\n",
            " 1.0581288  0.95853835 0.67225539 0.9048751  1.32732841 1.62504198\n",
            " 1.28181052]\n",
            "0.075\n",
            "0.05468029020892945\n",
            "0.057070845617796684\n",
            "0.05299152206115705\n",
            "0.05665117365449002\n",
            "0.05561089292304955\n",
            "0.059482413615164376\n",
            "0.0591651030692964\n",
            "0.060656702418944275\n",
            "0.05977836730751618\n",
            "0.05961366563459691\n",
            "0.05807233119090682\n",
            "0.05684403385378811\n",
            "0.05482663395466614\n",
            "0.05297861834339401\n",
            "0.050700350481436285\n",
            "0.048544621533150645\n",
            "0.04617882670615725\n",
            "0.04392360026047604\n",
            "0.04159178516883322\n",
            "0.03936907184599292\n",
            "0.03714858273330379\n",
            "0.035037008630959265\n",
            "0.03297211274840506\n",
            "0.031014232807517583\n",
            "0.029126092207441445\n",
            "0.027340861274826933\n",
            "0.025635311911104575\n",
            "0.024026761050430334\n",
            "0.022499972438983384\n",
            "0.02106309094812664\n",
            "0.019705509845985736\n",
            "0.018430146982824525\n",
            "0.017229157046424914\n",
            "0.016102556090937137\n",
            "0.015044218767861883\n",
            "0.014052624256281732\n",
            "0.013122781502050263\n",
            "0.012252427940538935\n",
            "0.011437371560884646\n",
            "0.010675065090962388\n",
            "0.009961917532600895\n",
            "0.00929535356137512\n",
            "0.00867226035460833\n",
            "0.00809017279738633\n",
            "0.0075463748473612036\n",
            "0.007038580343232237\n",
            "0.006564411846515572\n",
            "0.006121790534400429\n",
            "0.0057086328518138885\n",
            "0.005323072731357126\n",
            "0.004963284039041227\n",
            "0.004627606264862472\n",
            "0.0043144397268908286\n",
            "0.004022315794252496\n",
            "0.003749834274171082\n",
            "0.0034957018587555844\n",
            "0.003258694059490086\n",
            "0.003037675538403839\n",
            "0.00283157640940801\n",
            "0.0026394023219820095\n",
            "0.002460219243828554\n",
            "0.0022931578206337733\n",
            "0.00213740322952205\n",
            "0.0019921964077855847\n",
            "0.0018568269934091663\n",
            "0.0017306329017923172\n",
            "0.001612995192754152\n",
            "0.0015033368322927565\n",
            "0.001401118801000134\n",
            "0.001305838508540269\n",
            "0.001217026733744353\n",
            "0.0011342459475807143\n",
            "0.001057087835235837\n",
            "0.0009851716623552331\n",
            "0.0009181422229689318\n",
            "0.0008556683152487476\n",
            "0.0007974410143267718\n",
            "0.0007431722854840395\n",
            "0.0006925935139125983\n",
            "0.0006454542622744697\n",
            "0.000601521009583994\n",
            "0.0005605760490181577\n",
            "0.0005224164007000542\n",
            "0.0004868528402146577\n",
            "0.0004537089582485887\n",
            "0.0004228203079921222\n",
            "0.0003940335901171377\n",
            "0.00036720590670147324\n",
            "0.0003422040539418081\n",
            "0.00031890387060262704\n",
            "0.00029718962376024116\n",
            "0.0002769534406052987\n",
            "0.00025809477474769693\n",
            "0.00024051991120965788\n",
            "0.00022414150262628496\n",
            "0.0002088781383069449\n",
            "0.00019465394114703827\n",
            "0.0001813981926637176\n",
            "0.0001690449826625695\n",
            "0.00015753288311359803\n",
            "0.00014680464367344825\n",
            "0.00013680690812267748\n",
            "0.0001274899497634152\n",
            "0.00011880742491587881\n",
            "0.00011071614298252541\n",
            "0.00010317585220342984\n",
            "9.614903985721845e-05\n",
            "8.960074607777813e-05\n",
            "8.34983902582298e-05\n",
            "7.781160927056163e-05\n",
            "7.251210664338537e-05\n",
            "6.75735120014842e-05\n",
            "6.297125003706938e-05\n",
            "5.86824183939072e-05\n",
            "5.46856738425916e-05\n",
            "5.096112619429327e-05\n",
            "4.749023942363095e-05\n",
            "4.425573951660766e-05\n",
            "4.1241528580870934e-05\n",
            "3.843260479818866e-05\n",
            "3.5814987820126376e-05\n",
            "3.337564924099645e-05\n",
            "3.110244779436222e-05\n",
            "2.898406896623129e-05\n",
            "2.7009968706970255e-05\n",
            "2.5170320984369e-05\n",
            "2.3455968894525725e-05\n",
            "2.185837910828603e-05\n",
            "2.0369599413649908e-05\n",
            "1.898221914786724e-05\n",
            "1.7689332324710705e-05\n",
            "1.6484503269964534e-05\n",
            "1.5361734596153198e-05\n",
            "1.4315437358636274e-05\n",
            "1.3340403243411458e-05\n",
            "1.2431778642962854e-05\n",
            "1.1585040508910183e-05\n",
            "1.0795973835131534e-05\n",
            "1.006065068231976e-05\n",
            "9.375410629619552e-06\n",
            "[0.23529174 0.27153199 0.34560899 0.46639978 0.58782486 0.32393585\n",
            " 0.38182516 0.48644829 0.65157406 0.7393427  0.39285748 0.52055401\n",
            " 0.72704776 0.89233667 0.82098554 0.52445912 0.70897568 1.06862145\n",
            " 1.10137139 0.99993242 0.69557529 0.94118724 1.38708042 1.70335119\n",
            " 1.35214864]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:75: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:99: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}